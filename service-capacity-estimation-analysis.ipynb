{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":6357319,"sourceType":"datasetVersion","datasetId":3661740},{"sourceId":6417650,"sourceType":"datasetVersion","datasetId":3701604},{"sourceId":6429165,"sourceType":"datasetVersion","datasetId":3709467},{"sourceId":6897305,"sourceType":"datasetVersion","datasetId":3962024},{"sourceId":7849169,"sourceType":"datasetVersion","datasetId":4602778}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/aayushsin7a/service-capacity-estimation-analysis?scriptVersionId=179319312\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"!pip install pyspark","metadata":{"execution":{"iopub.status.busy":"2024-05-23T09:46:26.418398Z","iopub.execute_input":"2024-05-23T09:46:26.418801Z","iopub.status.idle":"2024-05-23T09:47:17.643566Z","shell.execute_reply.started":"2024-05-23T09:46:26.418766Z","shell.execute_reply":"2024-05-23T09:47:17.642307Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting pyspark\n  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: py4j==0.10.9.7 in /opt/conda/lib/python3.10/site-packages (from pyspark) (0.10.9.7)\nBuilding wheels for collected packages: pyspark\n  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488493 sha256=e61820cbadd7ecdfb8dd8d39bf80a606ded50a123bf16fc7182b9e262a661bce\n  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\nSuccessfully built pyspark\nInstalling collected packages: pyspark\nSuccessfully installed pyspark-3.5.1\n","output_type":"stream"}]},{"cell_type":"code","source":"import sys\nprint(sys.version)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-23T09:49:51.589791Z","iopub.execute_input":"2024-05-23T09:49:51.590259Z","iopub.status.idle":"2024-05-23T09:49:51.596899Z","shell.execute_reply.started":"2024-05-23T09:49:51.590199Z","shell.execute_reply":"2024-05-23T09:49:51.595616Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"3.10.13 | packaged by conda-forge | (main, Dec 23 2023, 15:36:39) [GCC 12.3.0]\n","output_type":"stream"}]},{"cell_type":"code","source":"import pyspark\nprint(pyspark.__version__)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-23T09:49:52.683426Z","iopub.execute_input":"2024-05-23T09:49:52.683815Z","iopub.status.idle":"2024-05-23T09:49:52.773362Z","shell.execute_reply.started":"2024-05-23T09:49:52.683788Z","shell.execute_reply":"2024-05-23T09:49:52.772094Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"3.5.1\n","output_type":"stream"}]},{"cell_type":"code","source":"from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.types import StringType\nfrom shapely.geometry import Point\nimport geopandas as gpd\nimport numpy as np\nfrom pyspark.sql.functions import *\nimport os\nimport sys\nfrom pyspark.sql.window import Window\nfrom pyspark.sql.functions import row_number\nfrom functools import reduce\nfrom pyspark.sql import DataFrame\nimport pyspark\n#sc=pyspark.SparkContext()\nfrom pyspark.context import SparkContext\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql import HiveContext\n#import sys\nfrom pyspark.sql.window import Window\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.functions import lead, lag, col ,datediff, to_date, lit, size , length , year, month, dayofmonth, from_unixtime, unix_timestamp, desc, asc, rank, min , expr, trim, date_format, avg\nimport datetime\nfrom datetime import datetime\nfrom pyspark.sql.functions import regexp_replace, col\nfrom pyspark.sql.types import IntegerType, DoubleType, StringType, DateType, TimestampType, StructField, NumericType\nfrom pyspark.sql.functions import isnan\nfrom pyspark.sql.functions import udf\nfrom io import BytesIO\n#import boto3\nimport math\nfrom pyspark.sql import Row\n#import pyarrow as pa\nimport pandas as pd\nfrom datetime import datetime, timedelta\n#from time import gmtime, strftime ,datetime\nfrom pyspark.sql.functions import concat, col, lit\nfrom pyspark.sql.functions import count, min,max\nfrom pyspark.sql.functions import col, when\n#import geohash2 as pgh\n#import pygeohash as pgh2\nspark = SparkSession.builder.appName('NE').getOrCreate()\nspark.sql(\"set spark.sql.legacy.timeParserPolicy=LEGACY\")","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2024-05-23T09:49:53.554115Z","iopub.execute_input":"2024-05-23T09:49:53.554989Z","iopub.status.idle":"2024-05-23T09:50:02.474172Z","shell.execute_reply.started":"2024-05-23T09:49:53.554953Z","shell.execute_reply":"2024-05-23T09:50:02.472817Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"Setting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n24/05/23 09:49:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n","output_type":"stream"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"DataFrame[key: string, value: string]"},"metadata":{}}]},{"cell_type":"code","source":"df_tele = spark.read.parquet(\"/kaggle/input/april-data/April.parquet\")","metadata":{"execution":{"iopub.status.busy":"2024-05-23T09:50:12.803025Z","iopub.execute_input":"2024-05-23T09:50:12.804414Z","iopub.status.idle":"2024-05-23T09:50:14.616268Z","shell.execute_reply.started":"2024-05-23T09:50:12.804367Z","shell.execute_reply":"2024-05-23T09:50:14.615272Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"                                                                                \r","output_type":"stream"}]},{"cell_type":"code","source":"df_tele.printSchema()","metadata":{"execution":{"iopub.status.busy":"2024-05-23T09:50:17.092202Z","iopub.execute_input":"2024-05-23T09:50:17.092616Z","iopub.status.idle":"2024-05-23T09:50:17.103783Z","shell.execute_reply.started":"2024-05-23T09:50:17.092586Z","shell.execute_reply":"2024-05-23T09:50:17.102491Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"root\n |-- Unnamed: 0: long (nullable = true)\n |-- vehicleId: string (nullable = true)\n |-- gpsLatitude: double (nullable = true)\n |-- gpsLongitude: double (nullable = true)\n |-- odometer_count: long (nullable = true)\n |-- District: string (nullable = true)\n |-- eventdate: timestamp_ntz (nullable = true)\n\n","output_type":"stream"}]},{"cell_type":"code","source":"#df_tele.select('vehicleId').distinct().count()","metadata":{"execution":{"iopub.status.busy":"2024-05-23T09:50:21.939182Z","iopub.execute_input":"2024-05-23T09:50:21.9396Z","iopub.status.idle":"2024-05-23T09:50:21.94461Z","shell.execute_reply.started":"2024-05-23T09:50:21.93957Z","shell.execute_reply":"2024-05-23T09:50:21.943296Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"1)Read telemetery\n2) vincount/total vincount based on gpslat,gpslong,1month\n3) District mapping\n4) Mode calculation -> if necessary \n4)region/sso/workshop data read \n5)Distance calculation\n7)s3 to save the file ( later append all 6 months ) ","metadata":{}},{"cell_type":"code","source":"df_tele = df_tele.drop(\"Unnamed: 0\")","metadata":{"execution":{"iopub.status.busy":"2024-05-23T09:50:56.422122Z","iopub.execute_input":"2024-05-23T09:50:56.422559Z","iopub.status.idle":"2024-05-23T09:50:56.473548Z","shell.execute_reply.started":"2024-05-23T09:50:56.422527Z","shell.execute_reply":"2024-05-23T09:50:56.472374Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"df_tele.show(5,False)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-23T09:50:59.005448Z","iopub.execute_input":"2024-05-23T09:50:59.005823Z","iopub.status.idle":"2024-05-23T09:51:03.947332Z","shell.execute_reply.started":"2024-05-23T09:50:59.005796Z","shell.execute_reply":"2024-05-23T09:51:03.94614Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"name":"stdout","text":"+-----------------+------------------+-----------------+--------------+------------+-------------------+\n|vehicleId        |gpsLatitude       |gpsLongitude     |odometer_count|District    |eventdate          |\n+-----------------+------------------+-----------------+--------------+------------+-------------------+\n|MAT373328J1P37808|18.90111329999999 |73.01143329999996|0             |Raigarh     |2023-04-01 00:00:00|\n|MAT373370G2F14206|17.88427489999998 |74.80197829999997|0             |Solapur     |2023-04-01 00:00:00|\n|MAT373370G2G16127|17.469103199999978|78.42705660000001|0             |Hydrabad    |2023-04-01 00:00:00|\n|MAT373370G2G16947|16.7756282        |78.13296489999999|0             |Mahabubnagar|2023-04-01 00:00:00|\n|MAT373370H2A00579|17.037436599999978|79.415955        |0             |Nalgonda    |2023-04-01 00:00:00|\n+-----------------+------------------+-----------------+--------------+------------+-------------------+\nonly showing top 5 rows\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# District Mapping","metadata":{}},{"cell_type":"code","source":"#districts = gpd.read_file(\"/kaggle/input/district-shapefiles/output.shp\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\n# District Mapping Function \ndef get_district(long, lat):\n    point = Point(long, lat)\n    for index, row in districts.iterrows():\n        if row['geometry'].contains(point):\n            return row['distname']\n    return 'District not found'\n'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Register the UDF\n#get_district_udf = udf(get_district, StringType())","metadata":{"execution":{"iopub.status.busy":"2024-05-23T09:51:33.336044Z","iopub.execute_input":"2024-05-23T09:51:33.336988Z","iopub.status.idle":"2024-05-23T09:51:33.341314Z","shell.execute_reply.started":"2024-05-23T09:51:33.336951Z","shell.execute_reply":"2024-05-23T09:51:33.340147Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"#df_tele = df_tele.withColumn('District', get_district_udf(df_tele['gpsLongitude'].cast('float'), df_tele['gpsLatitude'].cast('float')))\ndf_tele = df_tele.withColumn(\"eventdate\", to_date(df_tele[\"eventdate\"], \"yyyy-MM-dd\"))\ndf_tele = df_tele.withColumn('coordinates', concat(df_tele['gpsLatitude'], lit(','), df_tele['gpsLongitude']))\ndf_tele = df_tele.withColumn(\"eventdate\", to_date(df_tele[\"eventdate\"], \"yyyy-MM-dd\"))\n# Extract the month number from the event date\ndf_tele = df_tele.withColumn(\"month\", month(df_tele[\"eventdate\"]))","metadata":{"execution":{"iopub.status.busy":"2024-05-23T09:51:44.585927Z","iopub.execute_input":"2024-05-23T09:51:44.586343Z","iopub.status.idle":"2024-05-23T09:51:44.754144Z","shell.execute_reply.started":"2024-05-23T09:51:44.58631Z","shell.execute_reply":"2024-05-23T09:51:44.752739Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"#df_tele.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_region = spark.read.csv(\"/kaggle/input/workshops/all_region_workshops.csv\",header=True)","metadata":{"execution":{"iopub.status.busy":"2024-05-23T09:51:48.180623Z","iopub.execute_input":"2024-05-23T09:51:48.18148Z","iopub.status.idle":"2024-05-23T09:51:48.871833Z","shell.execute_reply.started":"2024-05-23T09:51:48.18144Z","shell.execute_reply":"2024-05-23T09:51:48.870565Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"df_region.show(3)\nprint(df_region.count())","metadata":{"execution":{"iopub.status.busy":"2024-05-23T09:51:51.006582Z","iopub.execute_input":"2024-05-23T09:51:51.00696Z","iopub.status.idle":"2024-05-23T09:51:51.971115Z","shell.execute_reply.started":"2024-05-23T09:51:51.006934Z","shell.execute_reply":"2024-05-23T09:51:51.969936Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"+--------------------+---------+----------+------+----------+\n|            Division|Lattitude| Longitude|   SSO|     State|\n+--------------------+---------+----------+------+----------+\n|100B520-Sv&Pa-Nag...|8.1908371|77.4192042| Salem|Tamil Nadu|\n|1001650-Sv&Pa-Nag...| 8.198264| 77.431443| Salem|Tamil Nadu|\n|2088645-Sv&Pa-Ney...|   8.3681|  77.08208|Cochin|    Kerala|\n+--------------------+---------+----------+------+----------+\nonly showing top 3 rows\n\n1592\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Python ","metadata":{}},{"cell_type":"code","source":"'''\nimport pandas as pd\nfrom openpyxl import load_workbook\nfrom math import radians, cos, sin, asin, sqrt\nfrom tqdm import tqdm\nfrom math import ceil\nimport numpy as np\nimport folium\nimport warnings\nimport plotly.graph_objects as go\nimport plotly.express as px\nimport matplotlib.pyplot as plt\nfrom scipy import stats as st\nfrom sklearn.cluster import KMeans\nfrom statistics import mode\nfrom geopy.geocoders import Nominatim\nfrom sklearn.cluster import DBSCAN\nfrom haversine import haversine\nfrom haversine import Unit\n\nwarnings.filterwarnings('ignore')\ngeolocator = Nominatim(user_agent=\"geoapiExercises\")\n'''\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\n\ndef cal_mode(data, latitude, longitude):\n    Returns : the first value if all the values are different\n\n\n    # Combining to find mode for Lat & Long both\n    data['combined'] = data[latitude] + ' - ' + data[longitude]\n\n    # Calculating mode\n    mod = data.groupby(['vehicleId', 'weekofMonth', 'District'])['combined'].apply(lambda x: mode(x)).reset_index() \n\n    for index, val in enumerate(mod['combined']):\n        mod.loc[index, 'Lat'] = val.split(' -')[0]\n        mod.loc[index, 'Long'] = val.split('- ')[1]\n\n    mod.drop('combined', axis=1, inplace=True)\n\n    return mod\n\n# def haversine(lonlat1, lonlat2):\n#     \"\"\"\n#     Calculate the great circle distance between two points\n#     on the earth (specified in decimal degrees)\n#     \"\"\"\n#     # convert decimal degrees to radians\n#     lat1, lon1 = lonlat1\n#     lat2, lon2 = lonlat2\n#     lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n#\n#     # haversine formula\n#     dlon = lon2 - lon1\n#     dlat = lat2 - lat1\n#     a = sin(dlat / 2) ** 2 + cos(lat1) * cos(lat2) * sin(dlon / 2) ** 2\n#     c = 2 * asin(sqrt(a))\n#     r = 6371  # Radius of earth in kilometers. Use 3956 for miles\n#     return c * r\n\ndef week_of_month(dt):\n    \"\"\" Returns the week of the month for the specified date.\"\"\"\n    first_day = dt.replace(day=1)\n\n    dom = dt.day\n    adjusted_dom = dom + first_day.weekday()\n\n    return int(ceil(adjusted_dom / 7.0))\n    \n'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#df_tele.count()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#df_filtered_region = df_region.filter(col(\"SSO\").isin(\"Delhi\", \"Lucknow\"))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_tele.printSchema()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#df_tele = df_tele.withColumn(\"eventdate\",to_date(col(\"eventdate\"),\"yyyy-MM-dd\"))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#df_tele.show(4)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import to_date, dayofmonth, floor\nfrom pyspark.sql.types import IntegerType\nfrom math import ceil\n\n# Initialize a Spark session\n#spark = SparkSession.builder.appName(\"WeekOfMonth\").getOrCreate()\n\n# Assuming you have a DataFrame named df_tele\n# Convert the \"eventdate\" column to date type\ndf_tele = df_tele.withColumn(\"eventdate\", to_date(df_tele[\"eventdate\"], \"yyyy-MM-dd\"))\n\n# Calculate the day of the month\ndf_tele = df_tele.withColumn(\"day_of_month\", dayofmonth(df_tele[\"eventdate\"]))\n\n# Define a UDF to calculate the week of the month\nfrom pyspark.sql.functions import udf\n\ndef week_of_month(dt):\n    \"\"\" Returns the week of the month for the specified date.\"\"\"\n    dom = dt.day\n    adjusted_dom = dom + (dt.weekday() + 1)  # Adjusted for 1-based indexing\n    return int(ceil(adjusted_dom / 7.0))\n\n# Register the UDF\nweek_udf = udf(week_of_month, IntegerType())\n\n# Calculate the week of the month\ndf_tele = df_tele.withColumn(\"week_of_month\", week_udf(df_tele[\"eventdate\"]))\n\n# Show the resulting DataFrame\ndf_tele.show(3)\n\n'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#df_tele = df_tele.withColumn(\"eventdate\", to_date(df_tele[\"eventdate\"], \"yyyy-MM-dd\"))\n\n# Extract the month number from the event date\n#df_tele = df_tele.withColumn(\"month\", month(df_tele[\"eventdate\"]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"from pyspark.sql.functions import year, month, weekofyear, dayofmonth, dayofweek, dayofyear, hour, minute, second, unix_timestamp\nfrom pyspark.sql import DataFrame\n\ndef add_datepart_spark(df: DataFrame, field_name: str, prefix=None, drop=True, time=False) -> DataFrame:\n    # Extract the date-related features using PySpark functions\n    df = df.withColumn(\"Year\", year(field_name))\n    df = df.withColumn(\"Month\", month(field_name))\n    df = df.withColumn(\"Week\", weekofyear(field_name))\n    df = df.withColumn(\"Day\", dayofmonth(field_name))\n    df = df.withColumn(\"Dayofweek\", dayofweek(field_name))\n    df = df.withColumn(\"Dayofyear\", dayofyear(field_name))\n    df = df.withColumn(\"Is_month_end\", (dayofmonth(field_name) == 31).cast(\"int\"))\n    df = df.withColumn(\"Is_month_start\", (dayofmonth(field_name) == 1).cast(\"int\"))\n    df = df.withColumn(\"Is_quarter_end\", (month(field_name) == 3).cast(\"int\"))\n    df = df.withColumn(\"Is_quarter_start\", (month(field_name) == 1).cast(\"int\"))\n    df = df.withColumn(\"Is_year_end\", (month(field_name) == 12).cast(\"int\"))\n    df = df.withColumn(\"Is_year_start\", (month(field_name) == 1).cast(\"int\"))\n\n    #if time:\n        #df = df.withColumn(\"Hour\", hour(field_name))\n        #df = df.withColumn(\"Minute\", minute(field_name))\n        #df = df.withColumn(\"Second\", second(field_name))\n\n    # Add the \"Elapsed\" column\n    #df = df.withColumn(\"Elapsed\", (unix_timestamp(field_name) * 1000).cast(\"long\"))\n\n    # Drop the original date column if drop is True\n    #if drop:\n        #df = df.drop(field_name)\n\n    return df\n\n# Usage:\ndf_tele = add_datepart_spark(df_tele, df_tele[\"eventdate\"], prefix='event')\n","metadata":{"execution":{"iopub.execute_input":"2023-09-05T09:22:04.521705Z","iopub.status.busy":"2023-09-05T09:22:04.521347Z","iopub.status.idle":"2023-09-05T09:22:04.705728Z","shell.execute_reply":"2023-09-05T09:22:04.704921Z","shell.execute_reply.started":"2023-09-05T09:22:04.521675Z"}}},{"cell_type":"code","source":"#df_tele.printSchema()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#df_tele.select(\"eventdate\",\"day_of_month\",\"week_of_month\",\"Dayofweek\",\"Dayofyear\").show(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\n# WEEK_OF_MONTH\nimport pyspark.sql.functions as F\n\ndef cal_mode(data, latitude, longitude):\n    # Combine latitude and longitude into a single column\n    data = data.withColumn('combined', F.concat_ws(' - ', data[latitude], data[longitude]))\n\n    # Calculate mode using a custom function\n    mode_udf = F.udf(lambda x: x.mode()[0] if len(x) > 0 else None)\n    mod = data.groupby(['vehicleId', 'week_of_month', 'District']).agg(mode_udf(F.collect_list('combined')).alias('mode'))\n\n    # Split the combined column into 'Lat' and 'Long'\n    mod = mod.withColumn('Lat', F.split(mod['mode'], ' - ')[0])\n    mod = mod.withColumn('Long', F.split(mod['mode'], ' - ')[1])\n\n    # Drop the intermediate 'combined' column\n    mod = mod.drop('mode')\n\n    return mod\n\n# This is where you define the df_mode variable\ndf_mode = cal_mode(df_tele, 'gpsLatitude', 'gpsLongitude')\n'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#print(df_mode.count())\n#df_mode.show(3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# WEEK OF MONTH","metadata":{}},{"cell_type":"code","source":"# WEEK OF MONTH\ndf_tele = df_tele.withColumn(\n    \"week_of_month\",\n    expr(\"ceil(dayofmonth(eventdate)/7)\")\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pyspark.sql.functions as F\n\ndef cal_mode_month(data, latitude, longitude):\n    # Combining latitude and longitude into a single column\n    data = data.withColumn('combined', F.concat_ws(' - ', data[latitude], data[longitude]))\n    # Calculating mode using PySpark's aggregation functions\n    mod = data.groupby(['vehicleId', 'month', 'District']) \\\n              .agg(F.expr('collect_list(combined) as combined_list')) \\\n              .withColumn('mode', F.expr('aggregate(combined_list, cast(null as string), (acc, x) -> IFNULL(acc, x), acc -> acc)'))\n    # Splitting the combined column into 'Lat' and 'Long'\n    mod = mod.withColumn('Lat', F.split(mod['mode'], ' - ')[0])\n    mod = mod.withColumn('Long', F.split(mod['mode'], ' - ')[1])\n    # Drop the intermediate columns\n    mod = mod.drop('combined', 'combined_list', 'mode')\n    return mod\n\n# This is where you define the df_mode variable\ndf_mode1 = cal_mode_month(df_tele, 'gpsLatitude', 'gpsLongitude')\n","metadata":{"execution":{"iopub.status.busy":"2024-05-23T09:52:41.304272Z","iopub.execute_input":"2024-05-23T09:52:41.304832Z","iopub.status.idle":"2024-05-23T09:52:41.638713Z","shell.execute_reply.started":"2024-05-23T09:52:41.30479Z","shell.execute_reply":"2024-05-23T09:52:41.63743Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"#df_mode1.count()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_mode1 = df_mode1.withColumn('Lat', col('Lat').cast(\"float\"))\n# Cast 'Long' column to Float\ndf_mode1 = df_mode1.withColumn('Long', col('Long').cast(\"float\"))\n# Cast 'District' column to String (assuming it's not already a string)\ndf_mode1 = df_mode1.withColumn('District', col('District').cast(\"string\"))","metadata":{"execution":{"iopub.status.busy":"2024-05-23T09:52:49.865267Z","iopub.execute_input":"2024-05-23T09:52:49.865672Z","iopub.status.idle":"2024-05-23T09:52:49.932126Z","shell.execute_reply.started":"2024-05-23T09:52:49.86564Z","shell.execute_reply":"2024-05-23T09:52:49.930872Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"df_mode1.columns","metadata":{"execution":{"iopub.status.busy":"2024-05-23T09:52:52.768283Z","iopub.execute_input":"2024-05-23T09:52:52.768687Z","iopub.status.idle":"2024-05-23T09:52:52.776999Z","shell.execute_reply.started":"2024-05-23T09:52:52.768656Z","shell.execute_reply":"2024-05-23T09:52:52.775777Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"['vehicleId', 'month', 'District', 'Lat', 'Long']"},"metadata":{}}]},{"cell_type":"code","source":"from pyspark.sql.functions import round\n# Assuming 'df_mode' is your PySpark DataFrame\n# Round 'Lat' column to two decimal places\ndf_mode1 = df_mode1.withColumn('Lat', round(df_mode1['Lat'], 2))\n# Round 'Long' column to two decimal places\ndf_mode1 = df_mode1.withColumn('Long', round(df_mode1['Long'], 2))\n","metadata":{"execution":{"iopub.status.busy":"2024-05-23T09:55:53.759172Z","iopub.execute_input":"2024-05-23T09:55:53.760228Z","iopub.status.idle":"2024-05-23T09:55:53.794589Z","shell.execute_reply.started":"2024-05-23T09:55:53.760173Z","shell.execute_reply":"2024-05-23T09:55:53.793218Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"\nfrom pyspark.sql import functions as F\n\n# Group by 'District' and count unique combinations of 'Lat' and 'Long'\nrou_off = df_mode1.groupBy('District', 'Lat', 'Long').agg(F.count(\"*\").alias(\"count\"))\n\n","metadata":{"execution":{"iopub.status.busy":"2024-05-23T09:56:12.207051Z","iopub.execute_input":"2024-05-23T09:56:12.20785Z","iopub.status.idle":"2024-05-23T09:56:12.254624Z","shell.execute_reply.started":"2024-05-23T09:56:12.207817Z","shell.execute_reply":"2024-05-23T09:56:12.253065Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"# Region ","metadata":{}},{"cell_type":"code","source":"df_region.columns","metadata":{"execution":{"iopub.status.busy":"2024-05-23T09:56:15.712745Z","iopub.execute_input":"2024-05-23T09:56:15.713126Z","iopub.status.idle":"2024-05-23T09:56:15.721731Z","shell.execute_reply.started":"2024-05-23T09:56:15.713097Z","shell.execute_reply":"2024-05-23T09:56:15.720354Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"['Division', 'Lattitude', 'Longitude', 'SSO', 'State']"},"metadata":{}}]},{"cell_type":"markdown","source":"# SSO","metadata":{}},{"cell_type":"code","source":"state_sso_dis = spark.read.csv(\"/kaggle/input/workshops/District to SSO master.csv\",header=True,inferSchema=True)","metadata":{"execution":{"iopub.status.busy":"2024-05-23T09:56:22.762271Z","iopub.execute_input":"2024-05-23T09:56:22.763038Z","iopub.status.idle":"2024-05-23T09:56:23.244714Z","shell.execute_reply.started":"2024-05-23T09:56:22.763002Z","shell.execute_reply":"2024-05-23T09:56:23.243571Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"state_sso_dis.show(4)","metadata":{"execution":{"iopub.status.busy":"2024-05-23T09:56:25.668008Z","iopub.execute_input":"2024-05-23T09:56:25.668445Z","iopub.status.idle":"2024-05-23T09:56:25.864774Z","shell.execute_reply.started":"2024-05-23T09:56:25.668412Z","shell.execute_reply":"2024-05-23T09:56:25.863725Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"+--------------------+---------+--------------+----------+\n|            District|vehicleId|         State|       SSO|\n+--------------------+---------+--------------+----------+\n|            Adilabad|        3|     Telangana| Hyderabad|\n|                Agra|       85| Uttar Pradesh|   Lucknow|\n|         Kadapa(YSR)|       40|Andhra Pradesh| Hyderabad|\n|Sri Potti Sriramu...|       32|Andhra Pradesh|Vijayawada|\n+--------------------+---------+--------------+----------+\nonly showing top 4 rows\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Join - sso with round off","metadata":{}},{"cell_type":"code","source":"#df_mode1 = rou_off.join(df_region_renamed, on='District', how='inner')\ndf_model = rou_off.join(state_sso_dis[['District','SSO','State']], on='District', how='inner')\n#print(df_mode1.count())","metadata":{"execution":{"iopub.status.busy":"2024-05-23T09:56:29.224833Z","iopub.execute_input":"2024-05-23T09:56:29.225191Z","iopub.status.idle":"2024-05-23T09:56:29.294187Z","shell.execute_reply.started":"2024-05-23T09:56:29.225164Z","shell.execute_reply":"2024-05-23T09:56:29.292829Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"df_model.select('SSO').distinct().show()","metadata":{"execution":{"iopub.status.busy":"2024-05-23T09:57:15.012768Z","iopub.execute_input":"2024-05-23T09:57:15.013166Z","iopub.status.idle":"2024-05-23T09:58:16.911226Z","shell.execute_reply.started":"2024-05-23T09:57:15.013136Z","shell.execute_reply":"2024-05-23T09:58:16.909964Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stderr","text":"[Stage 17:>                                                         (0 + 4) / 4]\r","output_type":"stream"},{"name":"stdout","text":"+-----------+\n|        SSO|\n+-----------+\n|  Bangalore|\n|    Udaipur|\n|Bhubaneswar|\n|      Jammu|\n|      Hubli|\n|      Patna|\n|    Lucknow|\n|    Chennai|\n|     Mumbai|\n|  Ahmedabad|\n|    Kolkata|\n|  Allahabad|\n|      Salem|\n|   Lucknow |\n|   Dehradun|\n|    Gurgaon|\n|       Pune|\n|      Delhi|\n|        JSR|\n|       #N/A|\n+-----------+\nonly showing top 20 rows\n\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"}]},{"cell_type":"code","source":"'''\nimport pyspark.sql.functions as F\n\ndef filter_df(df, sso_col, sso_list):\n    '''\n    Filters the DataFrame to only the rows where the sso column is in the sso_list\n    '''\n\n    # Creating a filter expression\n    filter_expr = F.col(sso_col).isin(sso_list)\n\n    # Filtering the DataFrame\n    df_filtered = df.filter(filter_expr)\n\n    return df_filtered\n'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#For all SSO\n# Assuming df_model is your DataFrame\n#unique_sso_values = df_model.select('SSO').distinct().rdd.map(lambda row: row[0]).collect()\n\n# Filtering the DataFrame\n#del_cha = filter_df(df_model, 'SSO', unique_sso_values)\n\n# Assuming df_model is already defined\nunique_sso_df = df_model.select('SSO').distinct()\n\n# Broadcast join for efficiency\ndel_cha = df_model.join(broadcast(unique_sso_df), on='SSO', how='inner')\n","metadata":{"execution":{"iopub.status.busy":"2024-05-23T09:59:46.909063Z","iopub.execute_input":"2024-05-23T09:59:46.910186Z","iopub.status.idle":"2024-05-23T09:59:47.019905Z","shell.execute_reply.started":"2024-05-23T09:59:46.910149Z","shell.execute_reply":"2024-05-23T09:59:47.018986Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"## delhi\n#work_reg = filter_df(df_region,'SSO',['Delhi', 'Lucknow'])\n#unique_sso_values_reg = df_region.select('SSO').distinct().rdd.map(lambda row: row[0]).collect()\n#work_reg = filter_df(df_region, 'SSO',unique_sso_values_reg) \n\nfrom pyspark.sql.functions import broadcast\n\n# Assuming df_region is already defined\nunique_sso_df_reg = df_region.select('SSO').distinct()\n# Broadcast join for efficiency\nwork_reg = df_region.join(broadcast(unique_sso_df_reg), on='SSO', how='inner')\n\n","metadata":{"execution":{"iopub.status.busy":"2024-05-23T10:00:10.464614Z","iopub.execute_input":"2024-05-23T10:00:10.465069Z","iopub.status.idle":"2024-05-23T10:00:10.507219Z","shell.execute_reply.started":"2024-05-23T10:00:10.465035Z","shell.execute_reply":"2024-05-23T10:00:10.506014Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"#work_reg.show(4)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#work_reg.count()\n# delhi / lucknow - count 92 ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del_cha.columns","metadata":{"execution":{"iopub.status.busy":"2024-05-23T10:00:14.12642Z","iopub.execute_input":"2024-05-23T10:00:14.126806Z","iopub.status.idle":"2024-05-23T10:00:14.136257Z","shell.execute_reply.started":"2024-05-23T10:00:14.126778Z","shell.execute_reply":"2024-05-23T10:00:14.134923Z"},"trusted":true},"execution_count":28,"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"['SSO', 'District', 'Lat', 'Long', 'count', 'State']"},"metadata":{}}]},{"cell_type":"code","source":"work_reg.columns","metadata":{"execution":{"iopub.status.busy":"2024-05-23T10:00:15.499429Z","iopub.execute_input":"2024-05-23T10:00:15.499819Z","iopub.status.idle":"2024-05-23T10:00:15.508292Z","shell.execute_reply.started":"2024-05-23T10:00:15.499792Z","shell.execute_reply":"2024-05-23T10:00:15.507025Z"},"trusted":true},"execution_count":29,"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"['SSO', 'Division', 'Lattitude', 'Longitude', 'State']"},"metadata":{}}]},{"cell_type":"code","source":"work_reg = work_reg.withColumnRenamed(\"Lattitude\", \"Latitude\")\n","metadata":{"execution":{"iopub.status.busy":"2024-05-23T10:00:19.353651Z","iopub.execute_input":"2024-05-23T10:00:19.354039Z","iopub.status.idle":"2024-05-23T10:00:19.370804Z","shell.execute_reply.started":"2024-05-23T10:00:19.354012Z","shell.execute_reply":"2024-05-23T10:00:19.369096Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"work_reg.columns","metadata":{"execution":{"iopub.status.busy":"2024-05-23T10:00:21.500203Z","iopub.execute_input":"2024-05-23T10:00:21.500657Z","iopub.status.idle":"2024-05-23T10:00:21.508559Z","shell.execute_reply.started":"2024-05-23T10:00:21.500625Z","shell.execute_reply":"2024-05-23T10:00:21.507405Z"},"trusted":true},"execution_count":31,"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"['SSO', 'Division', 'Latitude', 'Longitude', 'State']"},"metadata":{}}]},{"cell_type":"code","source":"del_cha.printSchema()","metadata":{"execution":{"iopub.status.busy":"2024-05-23T10:00:23.725251Z","iopub.execute_input":"2024-05-23T10:00:23.725708Z","iopub.status.idle":"2024-05-23T10:00:23.732393Z","shell.execute_reply.started":"2024-05-23T10:00:23.725677Z","shell.execute_reply":"2024-05-23T10:00:23.731096Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"root\n |-- SSO: string (nullable = true)\n |-- District: string (nullable = true)\n |-- Lat: float (nullable = true)\n |-- Long: float (nullable = true)\n |-- count: long (nullable = false)\n |-- State: string (nullable = true)\n\n","output_type":"stream"}]},{"cell_type":"code","source":"#del_cha = del_cha.withColumnRenamed(\"State\", \"state\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"work_reg.printSchema()","metadata":{"execution":{"iopub.status.busy":"2024-05-23T10:00:25.90734Z","iopub.execute_input":"2024-05-23T10:00:25.907722Z","iopub.status.idle":"2024-05-23T10:00:25.91485Z","shell.execute_reply.started":"2024-05-23T10:00:25.907695Z","shell.execute_reply":"2024-05-23T10:00:25.913678Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"root\n |-- SSO: string (nullable = true)\n |-- Division: string (nullable = true)\n |-- Latitude: string (nullable = true)\n |-- Longitude: string (nullable = true)\n |-- State: string (nullable = true)\n\n","output_type":"stream"}]},{"cell_type":"code","source":"'''\n\nfrom pyspark.sql.functions import col\n\n# Convert Latitude and Longitude columns to double type\nwork_reg = work_reg.withColumn(\"Latitude\", col(\"Latitude\").cast(\"double\"))\nwork_reg = work_reg.withColumn(\"Longitude\", col(\"Longitude\").cast(\"double\"))\n\n# Verify the updated schema\nwork_reg.printSchema()\n\n'''\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\n\ndef haversine(lat1, lon1, lat2, lon2):\n    \"\"\"\n    Calculates the Haversine distance between two points in latitude and longitude.\n    \n    Args:\n        lat1 (float): The latitude of the first point.\n        lon1 (float): The longitude of the first point.\n        lat2 (float): The latitude of the second point.\n        lon2 (float): The longitude of the second point.\n    \n    Returns:\n        float: The Haversine distance between the two points in kilometers.\n    \"\"\"\n    R = 6371.00885\n    dLat = (lat2 - lat1) * math.pi / 180.0\n    dLon = (lon2 - lon1) * math.pi / 180.0\n    a = math.sin(dLat / 2) ** 2 + math.cos(lat1 * math.pi / 180.0) * math.cos(lat2 * math.pi / 180.0) * math.sin(dLon / 2) ** 2\n    c = 2 * math.asin(math.sqrt(a))\n    return R * c\n'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\n\n\n# Register the UDF\nhaversine_udf = udf(haversine, DoubleType())\n\n# Cross join the DataFrames to create all combinations\nresult_df = del_cha.crossJoin(work_reg)\n\n# Calculate distances using the UDF and add them as a new column\n# Calculate distances using the UDF and add them as a new column\nresult_df = result_df.withColumn(\"Distance\", haversine_udf(\n    del_cha[\"Latitude\"], del_cha[\"Longitude\"],\n    work_reg[\"Latitude\"], work_reg[\"Longitude\"]\n))\n\n# Use window functions to find the minimum distance for each 'District'\nwindow_spec = Window.partitionBy(result_df[\"District\"]).orderBy(result_df[\"Distance\"])\nresult_df = result_df.withColumn(\"MinDistance\", spark_min(result_df[\"Distance\"]).over(window_spec))\n\n# Select the relevant columns (e.g., 'District', 'Division', and 'MinDistance')\nresult_df = result_df.select(\"District\", \"Division\", \"MinDistance\").distinct()\n\n# Show the resulting DataFrame\nresult_df.show()\n'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\n\n\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.window import Window\nfrom pyspark.sql.types import DoubleType\n\n# Define Haversine function\ndef haversine(lat1, lon1, lat2, lon2):\n    \"\"\"\n    Calculate the great circle distance between two points \n    on the earth (specified in decimal degrees)\n    \"\"\"\n    import math\n\n    # Convert latitude and longitude from degrees to radians\n    lat1, lon1, lat2, lon2 = map(math.radians, [lat1, lon1, lat2, lon2])\n\n    # Haversine formula\n    dlon = lon2 - lon1\n    dlat = lat2 - lat1\n    a = math.sin(dlat / 2) ** 2 + math.cos(lat1) * math.cos(lat2) * math.sin(dlon / 2) ** 2\n    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n    # Radius of the Earth in kilometers\n    R = 6371\n    distance = R * c\n\n    return distance\n\n# Register Haversine function as a UDF\nhaversine_udf = F.udf(haversine, DoubleType())\n\n# Cross join the DataFrames to create all combinations\nresult_df = del_cha.crossJoin(work_reg)\n\n# Calculate distances using the UDF and add them as a new column\nresult_df = result_df.withColumn(\"Distance\", haversine_udf(\n    del_cha[\"Latitude\"], del_cha[\"Longitude\"],\n    work_reg[\"Latitude\"], work_reg[\"Longitude\"]\n))\n\n# Use window functions to find the minimum distance for each 'District'\nwindow_spec = Window.partitionBy(result_df[\"District\"]).orderBy(result_df[\"Distance\"])\nresult_df = result_df.withColumn(\"MinDistance\", F.min(result_df[\"Distance\"]).over(window_spec))\n\n# Select the relevant columns (e.g., 'District', 'Division', and 'MinDistance')\nresult_df = result_df.select(del_cha[\"District\"], work_reg[\"SSO\"], del_cha[\"Latitude\"].alias(\"Del_Latitude\"), del_cha[\"Longitude\"].alias(\"Del_Longitude\"), del_cha[\"count\"], del_cha[\"State\"], (F.col(\"MinDistance\") * F.lit(1000)).alias(\"MinDistance\")).distinct()\nresult_df = result_df.orderBy(\"MinDistance\")\n# Show the resulting DataFrame\nresult_df.show()\n'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#result_df.show(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\ndef haversine(lat1, lon1, lat2, lon2):\n    import math\n    \n    # Convert latitude and longitude from degrees to radians\n    lat1, lon1, lat2, lon2 = map(math.radians, [lat1, lon1, lat2, lon2])\n    \n    # Haversine formula\n    dlon = lon2 - lon1\n    dlat = lat2 - lat1\n    a = math.sin(dlat / 2) ** 2 + math.cos(lat1) * math.cos(lat2) * math.sin(dlon / 2) ** 2\n    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n    \n    # Radius of the Earth in kilometers\n    R = 6371\n    distance = R * c\n    \n    return distance\n'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"''' \n#Register Haversine function as a UDF\nhaversine_udf = F.udf(haversine, DoubleType())\n\n# Cross join the DataFrames to create all combinations\nresult_df = del_cha.crossJoin(work_reg)\n\n# Calculate distances using the UDF and add them as a new column\nresult_df = result_df.withColumn(\"Distance\", haversine_udf(\n    del_cha[\"Latitude\"], del_cha[\"Longitude\"],\n    work_reg[\"Latitude\"], work_reg[\"Longitude\"]\n))\n\n# Use window functions to find the minimum distance for each 'District'\nwindow_spec = Window.partitionBy(result_df[\"District\"]).orderBy(result_df[\"Distance\"])\nresult_df = result_df.withColumn(\"MinDistance\", F.min(result_df[\"Distance\"]).over(window_spec))\n\n# Select the relevant columns (e.g., 'District', 'Division', and 'MinDistance')\nresult_df = result_df.select(del_cha[\"District\"], work_reg[\"SSO\"], del_cha[\"Latitude\"].alias(\"Del_Latitude\"), del_cha[\"Longitude\"].alias(\"Del_Longitude\"), del_cha[\"count\"], del_cha[\"State\"], (F.col(\"MinDistance\") * F.lit(1000)).alias(\"MinDistance\")).distinct()\nresult_df = result_df.orderBy(\"MinDistance\")\n# Show the resulting DataFrame\nresult_df.show()\n\n'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#result_df_filtered = result_df.filter(result_df[\"State\"] == \"Uttar Pradesh\")\n\n# Show the resulting filtered DataFrame\n#result_df_filtered.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#result_df_filtered.count()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rs = del_cha\nstation = work_reg","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rs.printSchema()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rs = rs.withColumn(\"Lat\", col(\"Lat\").cast(\"double\"))\nrs = rs.withColumn(\"Long\", col(\"Long\").cast(\"double\"))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"station.printSchema()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"station = station.withColumnRenamed(\"Latitude\", \"sLat\")\nstation = station.withColumnRenamed(\"Longitude\", \"sLong\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"station = station.withColumn(\"sLat\", col(\"sLat\").cast(\"double\"))\nstation = station.withColumn(\"sLong\", col(\"sLong\").cast(\"double\"))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"station.printSchema()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#from pyspark.sql.functions import F, lit\n\n# Haversine UDF (User-Defined Function)\ndef haversine_udf(lon1, lat1, lon2, lat2):\n  \"\"\"\n  UDF to calculate the great-circle distance between two points on a sphere using the Haversine formula.\n\n  Args:\n      lon1 (float): Longitude of the first point in degrees.\n      lat1 (float): Latitude of the first point in degrees.\n      lon2 (float): Longitude of the second point in degrees.\n      lat2 (float): Latitude of the second point in degrees.\n\n  Returns:\n      float: The distance between the two points in kilometers.\n  \"\"\"\n  # Convert decimal degrees to radians\n  lon1_rad = lon1 * F.lit(math.pi / 180.0)\n  lat1_rad = lat1 * F.lit(math.pi / 180.0)\n  lon2_rad = lon2 * F.lit(math.pi / 180.0)\n  lat2_rad = lat2 * F.lit(math.pi / 180.0)\n\n  # Haversine formula calculations\n  dlon = lon2_rad - lon1_rad\n  dlat = lat2_rad - lat1_rad\n  a = F.sin(dlat / 2) ** 2 + F.cos(lat1_rad) * F.cos(lat2_rad) * F.sin(dlon / 2) ** 2\n  c = 2 * F.asin(F.sqrt(a))\n  # Radius of Earth in kilometers (change to miles if needed)\n  R = lit(6371.0)\n\n  return c * R\n","metadata":{"execution":{"iopub.status.busy":"2024-05-23T10:01:09.05627Z","iopub.execute_input":"2024-05-23T10:01:09.05745Z","iopub.status.idle":"2024-05-23T10:01:09.065787Z","shell.execute_reply.started":"2024-05-23T10:01:09.057409Z","shell.execute_reply":"2024-05-23T10:01:09.064534Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"'''\nrs_with_distance = rs.withColumn(\n    \"distance_to_station\",\n    haversine_udf(\n        rs[\"Long\"],\n        rs[\"Lat\"],\n        F.lit(station.select(\"sLong\").head()[0]),  # Use F.lit() to convert Python value to column\n        F.lit(station.select(\"sLat\").head()[0])   # Use F.lit() to convert Python value to column\n    )\n)\n\n# Print the DataFrame with distance\n#rs_with_distance.show()\n\n'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rs_with_distance = rs_with_distance.orderBy(\"distance_to_station\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#rs_with_distance.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#print(result_df_filtered.count())\n#print(rs_with_distance.count())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del_cha.columns","metadata":{"execution":{"iopub.status.busy":"2024-05-23T10:02:05.702394Z","iopub.execute_input":"2024-05-23T10:02:05.702781Z","iopub.status.idle":"2024-05-23T10:02:05.709791Z","shell.execute_reply.started":"2024-05-23T10:02:05.702754Z","shell.execute_reply":"2024-05-23T10:02:05.708765Z"},"trusted":true},"execution_count":37,"outputs":[{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"['SSO', 'District', 'Lat', 'Long', 'count', 'State']"},"metadata":{}}]},{"cell_type":"code","source":"work_reg.columns","metadata":{"execution":{"iopub.status.busy":"2024-05-23T10:03:08.599718Z","iopub.execute_input":"2024-05-23T10:03:08.600113Z","iopub.status.idle":"2024-05-23T10:03:08.606819Z","shell.execute_reply.started":"2024-05-23T10:03:08.600085Z","shell.execute_reply":"2024-05-23T10:03:08.605739Z"},"trusted":true},"execution_count":39,"outputs":[{"execution_count":39,"output_type":"execute_result","data":{"text/plain":"['SSO', 'Division', 'Latitude', 'Longitude', 'State']"},"metadata":{}}]},{"cell_type":"raw","source":"work_reg.columns","metadata":{}},{"cell_type":"code","source":"\n\ndel_cha_distance = del_cha.withColumn(\n    \"Min_distance\",\n    haversine_udf(\n        del_cha[\"Lat\"],\n        del_cha[\"Long\"],\n        F.lit(work_reg.select(\"Longitude\").head()[0]),  # Use F.lit() to convert Python value to column\n        F.lit(work_reg.select(\"Latitude\").head()[0])   # Use F.lit() to convert Python value to column\n    )\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-23T10:03:29.319029Z","iopub.execute_input":"2024-05-23T10:03:29.319835Z","iopub.status.idle":"2024-05-23T10:03:30.159887Z","shell.execute_reply.started":"2024-05-23T10:03:29.3198Z","shell.execute_reply":"2024-05-23T10:03:30.158663Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"del_cha_distance.show()","metadata":{"execution":{"iopub.status.busy":"2024-05-23T10:03:38.915183Z","iopub.execute_input":"2024-05-23T10:03:38.915576Z","iopub.status.idle":"2024-05-23T10:04:46.011947Z","shell.execute_reply.started":"2024-05-23T10:03:38.915548Z","shell.execute_reply":"2024-05-23T10:04:46.010626Z"},"trusted":true},"execution_count":42,"outputs":[{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"name":"stdout","text":"+----------+---------------+-----+-----+-----+-------------+------------------+\n|       SSO|       District|  Lat| Long|count|        State|      Min_distance|\n+----------+---------------+-----+-----+-----+-------------+------------------+\n|   Lucknow|          Unnao|26.62| 80.7|  465|Uttar Pradesh| 8452.296540655547|\n| Bangalore|         Mandya|12.55|76.88|    3|    Karnataka| 8501.674200129422|\n|     Patna|      Begusarai| 25.5|85.94|   21|        Bihar| 8819.902824728599|\n|    Jaipur|         Jaipur|26.82| 75.5|   22|    Rajasthan| 8098.125789513314|\n|   Udaipur|        Jodhpur|27.16|73.15|    3|    Rajasthan| 7933.799897772404|\n|Chandigarh|           Moga|30.83|75.08|    4|       Punjab| 7980.661725422702|\n|     Patna|Kaimur (Bhabua)|25.22|83.51|   56|        Bihar| 8658.759288416753|\n|     Delhi|     South West|28.57|77.06|   54| NCT of Delhi| 8168.241279177264|\n|   Kolkata|          Haora|22.59|88.22|  559|  West Bengal| 8983.063668175757|\n|     Patna|     Kishanganj|26.37|88.07|   26|        Bihar| 8962.182181813321|\n|    Jaipur|         Nagaur|27.03|74.76|    6|    Rajasthan| 8044.000771027155|\n|     Patna|           Gaya|24.53|84.92|   59|        Bihar| 8758.542282880364|\n| Ahmedabad|          Kheda|22.74|72.66|   77|      Gujarat| 8022.542358723529|\n|   Gurgaon|        Bhiwani|28.84|75.98|    1|      Haryana|8087.2574942969795|\n| Ahmedabad|   Banas Kantha|24.61|71.74|   52|      Gujarat|  7913.73088159307|\n|   Gurgaon|         Rohtak|28.89|76.56|   67|      Haryana| 8126.893388302896|\n|       JSR|         Bokaro|23.63|86.29|   58|    Jharkhand| 8854.427078106844|\n|    Cochin|       Palakkad|10.65|76.57|    4|       Kerala| 8533.880931824668|\n|  Siliguri|     Jalpaiguri|26.54| 88.8|   94|  West Bengal| 9012.697964893776|\n|    Raipur|           Durg|21.15|81.31|    1|  Chhatisgarh| 8569.028988576903|\n+----------+---------------+-----+-----+-----+-------------+------------------+\nonly showing top 20 rows\n\n","output_type":"stream"}]},{"cell_type":"code","source":"#del_cha_distance.toPandas().to_csv(\"/kaggle/working/output.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#rs_with_distance.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del_cha = del_cha.filter(col(\"Long\").isNotNull() & col(\"Lat\").isNotNull())\nwork_reg = work_reg.filter(col(\"Longitude\").isNotNull() & col(\"Latitude\").isNotNull())\n","metadata":{"execution":{"iopub.status.busy":"2024-05-23T10:25:06.422237Z","iopub.execute_input":"2024-05-23T10:25:06.422795Z","iopub.status.idle":"2024-05-23T10:25:06.457751Z","shell.execute_reply.started":"2024-05-23T10:25:06.422758Z","shell.execute_reply":"2024-05-23T10:25:06.456605Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"import math\nfrom pyspark.sql.functions import udf, col\nfrom pyspark.sql.types import DoubleType\n\ndef haversine(lon1, lat1, lon2, lat2):\n    if None in [lon1, lat1, lon2, lat2]:\n        return None\n    lon1_rad = lon1 * (math.pi / 180.0)\n    lat1_rad = lat1 * (math.pi / 180.0)\n    lon2_rad = lon2 * (math.pi / 180.0)\n    lat2_rad = lat2 * (math.pi / 180.0)\n    dlon = lon2_rad - lon1_rad\n    dlat = lat2_rad - lat1_rad\n    a = (math.sin(dlat / 2) ** 2) + math.cos(lat1_rad) * math.cos(lat2_rad) * (math.sin(dlon / 2) ** 2)\n    c = 2 * math.asin(math.sqrt(a))\n    R = 6371.0  # Radius of Earth in kilometers\n    return c * R\n\nhaversine_udf = udf(haversine, DoubleType())\n","metadata":{"execution":{"iopub.status.busy":"2024-05-23T10:25:23.635019Z","iopub.execute_input":"2024-05-23T10:25:23.635417Z","iopub.status.idle":"2024-05-23T10:25:23.644961Z","shell.execute_reply.started":"2024-05-23T10:25:23.635388Z","shell.execute_reply":"2024-05-23T10:25:23.64358Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"haversine_udf = udf(haversine, DoubleType())","metadata":{"execution":{"iopub.status.busy":"2024-05-23T10:25:37.023095Z","iopub.execute_input":"2024-05-23T10:25:37.023508Z","iopub.status.idle":"2024-05-23T10:25:37.028836Z","shell.execute_reply.started":"2024-05-23T10:25:37.023476Z","shell.execute_reply":"2024-05-23T10:25:37.027689Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"code","source":"broadcast_station = broadcast(work_reg)","metadata":{"execution":{"iopub.status.busy":"2024-05-23T10:25:55.32778Z","iopub.execute_input":"2024-05-23T10:25:55.328233Z","iopub.status.idle":"2024-05-23T10:25:55.339748Z","shell.execute_reply.started":"2024-05-23T10:25:55.328186Z","shell.execute_reply":"2024-05-23T10:25:55.338648Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"code","source":"del_cha_distance = del_cha.crossJoin(broadcast_station) \\\n    .withColumn(\"distance_to_station\", haversine_udf(col(\"Long\"), col(\"Lat\"), col(\"Longitude\"), col(\"Latitude\")))","metadata":{"execution":{"iopub.status.busy":"2024-05-23T10:26:16.21516Z","iopub.execute_input":"2024-05-23T10:26:16.215753Z","iopub.status.idle":"2024-05-23T10:26:16.269064Z","shell.execute_reply.started":"2024-05-23T10:26:16.21571Z","shell.execute_reply":"2024-05-23T10:26:16.267761Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"code","source":"del_cha_distance = del_cha_distance.orderBy(\"distance_to_station\")","metadata":{"execution":{"iopub.status.busy":"2024-05-23T10:26:29.269893Z","iopub.execute_input":"2024-05-23T10:26:29.270938Z","iopub.status.idle":"2024-05-23T10:26:29.288445Z","shell.execute_reply.started":"2024-05-23T10:26:29.270887Z","shell.execute_reply":"2024-05-23T10:26:29.286917Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"code","source":"del_cha_distance.show()","metadata":{"execution":{"iopub.status.busy":"2024-05-23T10:26:36.155959Z","iopub.execute_input":"2024-05-23T10:26:36.156427Z","iopub.status.idle":"2024-05-23T10:27:48.324119Z","shell.execute_reply.started":"2024-05-23T10:26:36.156394Z","shell.execute_reply":"2024-05-23T10:27:48.322652Z"},"trusted":true},"execution_count":62,"outputs":[{"name":"stderr","text":"24/05/23 10:27:48 ERROR Executor: Exception in task 1.0 in stage 133.0 (TID 167)\norg.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/tmp/ipykernel_33/4181433418.py\", line 10, in haversine\nTypeError: can't multiply sequence by non-int of type 'float'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:94)\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage13.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator.isEmpty(Iterator.scala:387)\n\tat scala.collection.Iterator.isEmpty$(Iterator.scala:387)\n\tat scala.collection.AbstractIterator.isEmpty(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.nonEmpty(TraversableOnce.scala:143)\n\tat scala.collection.TraversableOnce.nonEmpty$(TraversableOnce.scala:143)\n\tat scala.collection.AbstractIterator.nonEmpty(Iterator.scala:1431)\n\tat org.apache.spark.rdd.RDD.$anonfun$takeOrdered$2(RDD.scala:1559)\n\tat org.apache.spark.rdd.RDD.$anonfun$takeOrdered$2$adapted(RDD.scala:1558)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2(RDD.scala:910)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2$adapted(RDD.scala:910)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n24/05/23 10:27:48 WARN TaskSetManager: Lost task 1.0 in stage 133.0 (TID 167) (7d6dd15667e7 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/tmp/ipykernel_33/4181433418.py\", line 10, in haversine\nTypeError: can't multiply sequence by non-int of type 'float'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:94)\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage13.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator.isEmpty(Iterator.scala:387)\n\tat scala.collection.Iterator.isEmpty$(Iterator.scala:387)\n\tat scala.collection.AbstractIterator.isEmpty(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.nonEmpty(TraversableOnce.scala:143)\n\tat scala.collection.TraversableOnce.nonEmpty$(TraversableOnce.scala:143)\n\tat scala.collection.AbstractIterator.nonEmpty(Iterator.scala:1431)\n\tat org.apache.spark.rdd.RDD.$anonfun$takeOrdered$2(RDD.scala:1559)\n\tat org.apache.spark.rdd.RDD.$anonfun$takeOrdered$2$adapted(RDD.scala:1558)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2(RDD.scala:910)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2$adapted(RDD.scala:910)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\n24/05/23 10:27:48 ERROR TaskSetManager: Task 1 in stage 133.0 failed 1 times; aborting job\n24/05/23 10:27:48 WARN TaskSetManager: Lost task 2.0 in stage 133.0 (TID 168) (7d6dd15667e7 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 1 in stage 133.0 failed 1 times, most recent failure: Lost task 1.0 in stage 133.0 (TID 167) (7d6dd15667e7 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/tmp/ipykernel_33/4181433418.py\", line 10, in haversine\nTypeError: can't multiply sequence by non-int of type 'float'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:94)\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage13.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator.isEmpty(Iterator.scala:387)\n\tat scala.collection.Iterator.isEmpty$(Iterator.scala:387)\n\tat scala.collection.AbstractIterator.isEmpty(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.nonEmpty(TraversableOnce.scala:143)\n\tat scala.collection.TraversableOnce.nonEmpty$(TraversableOnce.scala:143)\n\tat scala.collection.AbstractIterator.nonEmpty(Iterator.scala:1431)\n\tat org.apache.spark.rdd.RDD.$anonfun$takeOrdered$2(RDD.scala:1559)\n\tat org.apache.spark.rdd.RDD.$anonfun$takeOrdered$2$adapted(RDD.scala:1558)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2(RDD.scala:910)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2$adapted(RDD.scala:910)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:)\n24/05/23 10:27:48 WARN TaskSetManager: Lost task 3.0 in stage 133.0 (TID 169) (7d6dd15667e7 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 1 in stage 133.0 failed 1 times, most recent failure: Lost task 1.0 in stage 133.0 (TID 167) (7d6dd15667e7 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/tmp/ipykernel_33/4181433418.py\", line 10, in haversine\nTypeError: can't multiply sequence by non-int of type 'float'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:94)\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage13.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator.isEmpty(Iterator.scala:387)\n\tat scala.collection.Iterator.isEmpty$(Iterator.scala:387)\n\tat scala.collection.AbstractIterator.isEmpty(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.nonEmpty(TraversableOnce.scala:143)\n\tat scala.collection.TraversableOnce.nonEmpty$(TraversableOnce.scala:143)\n\tat scala.collection.AbstractIterator.nonEmpty(Iterator.scala:1431)\n\tat org.apache.spark.rdd.RDD.$anonfun$takeOrdered$2(RDD.scala:1559)\n\tat org.apache.spark.rdd.RDD.$anonfun$takeOrdered$2$adapted(RDD.scala:1558)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2(RDD.scala:910)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2$adapted(RDD.scala:910)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:)\n24/05/23 10:27:48 WARN TaskSetManager: Lost task 0.0 in stage 133.0 (TID 166) (7d6dd15667e7 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 1 in stage 133.0 failed 1 times, most recent failure: Lost task 1.0 in stage 133.0 (TID 167) (7d6dd15667e7 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/tmp/ipykernel_33/4181433418.py\", line 10, in haversine\nTypeError: can't multiply sequence by non-int of type 'float'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:94)\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage13.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator.isEmpty(Iterator.scala:387)\n\tat scala.collection.Iterator.isEmpty$(Iterator.scala:387)\n\tat scala.collection.AbstractIterator.isEmpty(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.nonEmpty(TraversableOnce.scala:143)\n\tat scala.collection.TraversableOnce.nonEmpty$(TraversableOnce.scala:143)\n\tat scala.collection.AbstractIterator.nonEmpty(Iterator.scala:1431)\n\tat org.apache.spark.rdd.RDD.$anonfun$takeOrdered$2(RDD.scala:1559)\n\tat org.apache.spark.rdd.RDD.$anonfun$takeOrdered$2$adapted(RDD.scala:1558)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2(RDD.scala:910)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2$adapted(RDD.scala:910)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:)\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mPythonException\u001b[0m                           Traceback (most recent call last)","Cell \u001b[0;32mIn[62], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdel_cha_distance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pyspark/sql/dataframe.py:945\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    885\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mshow\u001b[39m(\u001b[38;5;28mself\u001b[39m, n: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m, truncate: Union[\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, vertical: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    886\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Prints the first ``n`` rows to the console.\u001b[39;00m\n\u001b[1;32m    887\u001b[0m \n\u001b[1;32m    888\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    943\u001b[0m \u001b[38;5;124;03m    name | Bob\u001b[39;00m\n\u001b[1;32m    944\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 945\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_show_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pyspark/sql/dataframe.py:963\u001b[0m, in \u001b[0;36mDataFrame._show_string\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    957\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m    958\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    959\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m    960\u001b[0m     )\n\u001b[1;32m    962\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[0;32m--> 963\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    964\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    965\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n","\u001b[0;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/tmp/ipykernel_33/4181433418.py\", line 10, in haversine\nTypeError: can't multiply sequence by non-int of type 'float'\n"],"ename":"PythonException","evalue":"\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/tmp/ipykernel_33/4181433418.py\", line 10, in haversine\nTypeError: can't multiply sequence by non-int of type 'float'\n","output_type":"error"}]},{"cell_type":"code","source":"import math\nfrom pyspark.sql.functions import udf, col, broadcast\nfrom pyspark.sql.types import DoubleType\n\ndef haversine(lon1, lat1, lon2, lat2):\n    try:\n        lon1 = float(lon1)\n        lat1 = float(lat1)\n        lon2 = float(lon2)\n        lat2 = float(lat2)\n    except (TypeError, ValueError):\n        return None\n\n    if None in [lon1, lat1, lon2, lat2]:\n        return None\n\n    lon1_rad = lon1 * (math.pi / 180.0)\n    lat1_rad = lat1 * (math.pi / 180.0)\n    lon2_rad = lon2 * (math.pi / 180.0)\n    lat2_rad = lat2 * (math.pi / 180.0)\n    dlon = lon2_rad - lon1_rad\n    dlat = lat2_rad - lat1_rad\n    a = (math.sin(dlat / 2) ** 2) + math.cos(lat1_rad) * math.cos(lat2_rad) * (math.sin(dlon / 2) ** 2)\n    c = 2 * math.asin(math.sqrt(a))\n    R = 6371.0  # Radius of Earth in kilometers\n    return c * R\n","metadata":{"execution":{"iopub.status.busy":"2024-05-23T10:30:02.975662Z","iopub.execute_input":"2024-05-23T10:30:02.97614Z","iopub.status.idle":"2024-05-23T10:30:02.985734Z","shell.execute_reply.started":"2024-05-23T10:30:02.976106Z","shell.execute_reply":"2024-05-23T10:30:02.984276Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"code","source":"haversine_udf = udf(haversine, DoubleType())","metadata":{"execution":{"iopub.status.busy":"2024-05-23T10:30:17.645348Z","iopub.execute_input":"2024-05-23T10:30:17.645756Z","iopub.status.idle":"2024-05-23T10:30:17.652072Z","shell.execute_reply.started":"2024-05-23T10:30:17.645726Z","shell.execute_reply":"2024-05-23T10:30:17.650665Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"code","source":"# Filter out rows with None values for longitude and latitude\ndel_cha = del_cha.filter(col(\"Long\").isNotNull() & col(\"Lat\").isNotNull())\nwork_reg = work_reg.filter(col(\"Longitude\").isNotNull() & col(\"Latitude\").isNotNull())\n","metadata":{"execution":{"iopub.status.busy":"2024-05-23T10:30:26.888258Z","iopub.execute_input":"2024-05-23T10:30:26.888655Z","iopub.status.idle":"2024-05-23T10:30:26.921037Z","shell.execute_reply.started":"2024-05-23T10:30:26.888626Z","shell.execute_reply":"2024-05-23T10:30:26.919885Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"code","source":"# Explicitly cast the columns to float\ndel_cha = del_cha.withColumn(\"Long\", col(\"Long\").cast(\"float\"))\ndel_cha = del_cha.withColumn(\"Lat\", col(\"Lat\").cast(\"float\"))\nwork_reg = work_reg.withColumn(\"Longitude\", col(\"Longitude\").cast(\"float\"))\nwork_reg = work_reg.withColumn(\"Latitude\", col(\"Latitude\").cast(\"float\"))\n","metadata":{"execution":{"iopub.status.busy":"2024-05-23T10:30:39.067057Z","iopub.execute_input":"2024-05-23T10:30:39.067479Z","iopub.status.idle":"2024-05-23T10:30:39.114136Z","shell.execute_reply.started":"2024-05-23T10:30:39.067444Z","shell.execute_reply":"2024-05-23T10:30:39.113201Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"code","source":"# Broadcast the station DataFrame\nbroadcast_station = broadcast(work_reg)","metadata":{"execution":{"iopub.status.busy":"2024-05-23T10:30:51.458811Z","iopub.execute_input":"2024-05-23T10:30:51.459218Z","iopub.status.idle":"2024-05-23T10:30:51.468012Z","shell.execute_reply.started":"2024-05-23T10:30:51.459174Z","shell.execute_reply":"2024-05-23T10:30:51.466927Z"},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"code","source":"# Perform join and calculate distances\ndel_cha_distance = del_cha.crossJoin(broadcast_station) \\\n    .withColumn(\"distance_to_station\", haversine_udf(col(\"Long\"), col(\"Lat\"), col(\"Longitude\"), col(\"Latitude\")))\n","metadata":{"execution":{"iopub.status.busy":"2024-05-23T10:30:59.718627Z","iopub.execute_input":"2024-05-23T10:30:59.719035Z","iopub.status.idle":"2024-05-23T10:30:59.767015Z","shell.execute_reply.started":"2024-05-23T10:30:59.718996Z","shell.execute_reply":"2024-05-23T10:30:59.765826Z"},"trusted":true},"execution_count":68,"outputs":[]},{"cell_type":"code","source":"\n# Order by distance\ndel_cha_distance = del_cha_distance.orderBy(\"distance_to_station\")","metadata":{"execution":{"iopub.status.busy":"2024-05-23T10:42:31.249019Z","iopub.execute_input":"2024-05-23T10:42:31.250243Z","iopub.status.idle":"2024-05-23T10:42:31.338588Z","shell.execute_reply.started":"2024-05-23T10:42:31.250172Z","shell.execute_reply":"2024-05-23T10:42:31.337398Z"},"trusted":true},"execution_count":71,"outputs":[]},{"cell_type":"code","source":"del_cha_distance.show(10,False)","metadata":{"execution":{"iopub.status.busy":"2024-05-23T10:48:47.516013Z","iopub.execute_input":"2024-05-23T10:48:47.516549Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"[Stage 153:>  (0 + 4) / 4][Stage 154:>  (0 + 0) / 4][Stage 155:>  (0 + 0) / 1]\r","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}