{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install pyspark ","metadata":{"execution":{"iopub.status.busy":"2024-01-17T08:41:11.393502Z","iopub.execute_input":"2024-01-17T08:41:11.394177Z","iopub.status.idle":"2024-01-17T08:42:03.094621Z","shell.execute_reply.started":"2024-01-17T08:41:11.394136Z","shell.execute_reply":"2024-01-17T08:42:03.093265Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting pyspark\n  Downloading pyspark-3.5.0.tar.gz (316.9 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: py4j==0.10.9.7 in /opt/conda/lib/python3.10/site-packages (from pyspark) (0.10.9.7)\nBuilding wheels for collected packages: pyspark\n  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.5.0-py2.py3-none-any.whl size=317425350 sha256=f25b7996e7be912c4be1bd2a4875e303f9c304a742a1541b4ee5c27aa7ed117d\n  Stored in directory: /root/.cache/pip/wheels/41/4e/10/c2cf2467f71c678cfc8a6b9ac9241e5e44a01940da8fbb17fc\nSuccessfully built pyspark\nInstalling collected packages: pyspark\nSuccessfully installed pyspark-3.5.0\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nfrom pyspark.sql.functions import *\nimport os\nimport sys\nfrom pyspark.sql.window import Window\nfrom pyspark.sql.functions import row_number\nfrom functools import reduce\nfrom pyspark.sql import DataFrame\nimport pyspark\n#sc=pyspark.SparkContext()\nfrom pyspark.context import SparkContext\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql import HiveContext\n#import sys\nfrom pyspark.sql.window import Window\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.functions import lead, lag, col ,datediff, to_date, lit, size , length , year, month, dayofmonth, from_unixtime, unix_timestamp, desc, asc, rank, min , expr, trim, date_format, avg\nimport datetime\nfrom datetime import datetime\nfrom pyspark.sql.functions import regexp_replace, col\nfrom pyspark.sql.types import IntegerType, DoubleType, StringType, DateType, TimestampType, StructField, NumericType\nfrom pyspark.sql.functions import isnan\nfrom pyspark.sql.functions import udf\nfrom io import BytesIO\n#import boto3\nimport math\nfrom pyspark.sql import Row\n#import pyarrow as pa\nimport pandas as pd\nfrom datetime import datetime, timedelta\n#from time import gmtime, strftime ,datetime\nfrom pyspark.sql.functions import concat, col, lit\nfrom pyspark.sql.functions import count, min,max\nfrom pyspark.sql.functions import col, when\n#import geohash2 as pgh\n#import pygeohash as pgh2\nspark = SparkSession.builder.appName('NE').getOrCreate()\nspark.sql(\"set spark.sql.legacy.timeParserPolicy=LEGACY\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-01-17T08:42:03.097190Z","iopub.execute_input":"2024-01-17T08:42:03.097593Z","iopub.status.idle":"2024-01-17T08:42:11.918367Z","shell.execute_reply.started":"2024-01-17T08:42:03.097554Z","shell.execute_reply":"2024-01-17T08:42:11.917135Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"Setting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n24/01/17 08:42:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"DataFrame[key: string, value: string]"},"metadata":{}}]},{"cell_type":"code","source":"df_tele = spark.read.parquet(\"/kaggle/input/January/January.parquet\")","metadata":{"execution":{"iopub.status.busy":"2024-01-17T08:42:11.921191Z","iopub.execute_input":"2024-01-17T08:42:11.922224Z","iopub.status.idle":"2024-01-17T08:42:13.990759Z","shell.execute_reply.started":"2024-01-17T08:42:11.922161Z","shell.execute_reply":"2024-01-17T08:42:13.989194Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"                                                                                \r","output_type":"stream"}]},{"cell_type":"code","source":"df_region = spark.read.csv(\"/kaggle/input/network-expansion/all_region_workshops.csv\",header=True)","metadata":{"execution":{"iopub.status.busy":"2024-01-17T08:42:13.992153Z","iopub.execute_input":"2024-01-17T08:42:13.992585Z","iopub.status.idle":"2024-01-17T08:42:17.142549Z","shell.execute_reply.started":"2024-01-17T08:42:13.992546Z","shell.execute_reply":"2024-01-17T08:42:17.141352Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"#df_tele.show(3)","metadata":{"execution":{"iopub.status.busy":"2024-01-17T08:42:17.150285Z","iopub.execute_input":"2024-01-17T08:42:17.153051Z","iopub.status.idle":"2024-01-17T08:42:17.160378Z","shell.execute_reply.started":"2024-01-17T08:42:17.152983Z","shell.execute_reply":"2024-01-17T08:42:17.158983Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"#df_region.show(3)\nprint(df_region.count())","metadata":{"execution":{"iopub.status.busy":"2024-01-17T08:42:17.164045Z","iopub.execute_input":"2024-01-17T08:42:17.166332Z","iopub.status.idle":"2024-01-17T08:42:18.117473Z","shell.execute_reply.started":"2024-01-17T08:42:17.166289Z","shell.execute_reply":"2024-01-17T08:42:18.116203Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"1592\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Python ","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom openpyxl import load_workbook\nfrom math import radians, cos, sin, asin, sqrt\nfrom tqdm import tqdm\nfrom math import ceil\nimport numpy as np\nimport folium\nimport warnings\nimport plotly.graph_objects as go\nimport plotly.express as px\nimport matplotlib.pyplot as plt\nfrom scipy import stats as st\nfrom sklearn.cluster import KMeans\nfrom statistics import mode\nfrom geopy.geocoders import Nominatim\nfrom sklearn.cluster import DBSCAN\nfrom haversine import haversine\nfrom haversine import Unit\n\nwarnings.filterwarnings('ignore')\ngeolocator = Nominatim(user_agent=\"geoapiExercises\")","metadata":{"execution":{"iopub.status.busy":"2024-01-17T08:42:18.118857Z","iopub.execute_input":"2024-01-17T08:42:18.119311Z","iopub.status.idle":"2024-01-17T08:42:22.412346Z","shell.execute_reply.started":"2024-01-17T08:42:18.119259Z","shell.execute_reply":"2024-01-17T08:42:22.410800Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def cal_mode(data, latitude, longitude):\n    '''\n    Returns : the first value if all the values are different\n    '''\n\n    # Combining to find mode for Lat & Long both\n    data['combined'] = data[latitude] + ' - ' + data[longitude]\n\n    # Calculating mode\n    mod = data.groupby(['vehicleId', 'weekofMonth', 'District'])['combined'].apply(lambda x: mode(x)).reset_index()\n\n    for index, val in enumerate(mod['combined']):\n        mod.loc[index, 'Lat'] = val.split(' -')[0]\n        mod.loc[index, 'Long'] = val.split('- ')[1]\n\n    mod.drop('combined', axis=1, inplace=True)\n\n    return mod\n\n# def haversine(lonlat1, lonlat2):\n#     \"\"\"\n#     Calculate the great circle distance between two points\n#     on the earth (specified in decimal degrees)\n#     \"\"\"\n#     # convert decimal degrees to radians\n#     lat1, lon1 = lonlat1\n#     lat2, lon2 = lonlat2\n#     lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n#\n#     # haversine formula\n#     dlon = lon2 - lon1\n#     dlat = lat2 - lat1\n#     a = sin(dlat / 2) ** 2 + cos(lat1) * cos(lat2) * sin(dlon / 2) ** 2\n#     c = 2 * asin(sqrt(a))\n#     r = 6371  # Radius of earth in kilometers. Use 3956 for miles\n#     return c * r\n\ndef week_of_month(dt):\n    \"\"\" Returns the week of the month for the specified date.\"\"\"\n    first_day = dt.replace(day=1)\n\n    dom = dt.day\n    adjusted_dom = dom + first_day.weekday()\n\n    return int(ceil(adjusted_dom / 7.0))","metadata":{"execution":{"iopub.status.busy":"2024-01-17T08:42:22.413998Z","iopub.execute_input":"2024-01-17T08:42:22.414884Z","iopub.status.idle":"2024-01-17T08:42:22.425974Z","shell.execute_reply.started":"2024-01-17T08:42:22.414846Z","shell.execute_reply":"2024-01-17T08:42:22.423651Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"df_tele.count()","metadata":{"execution":{"iopub.status.busy":"2024-01-17T08:42:22.427557Z","iopub.execute_input":"2024-01-17T08:42:22.427896Z","iopub.status.idle":"2024-01-17T08:42:22.946291Z","shell.execute_reply.started":"2024-01-17T08:42:22.427866Z","shell.execute_reply":"2024-01-17T08:42:22.945270Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"8535257"},"metadata":{}}]},{"cell_type":"code","source":"#df_filtered_region = df_region.filter(col(\"SSO\").isin(\"Delhi\", \"Lucknow\"))","metadata":{"execution":{"iopub.status.busy":"2024-01-17T08:42:22.949339Z","iopub.execute_input":"2024-01-17T08:42:22.949658Z","iopub.status.idle":"2024-01-17T08:42:22.955408Z","shell.execute_reply.started":"2024-01-17T08:42:22.949631Z","shell.execute_reply":"2024-01-17T08:42:22.954010Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"#df_tele.printSchema()","metadata":{"execution":{"iopub.status.busy":"2024-01-17T08:42:22.957708Z","iopub.execute_input":"2024-01-17T08:42:22.958820Z","iopub.status.idle":"2024-01-17T08:42:22.964467Z","shell.execute_reply.started":"2024-01-17T08:42:22.958778Z","shell.execute_reply":"2024-01-17T08:42:22.963552Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"df_tele = df_tele.withColumn(\"eventdate\",to_date(col(\"eventdate\"),\"yyyy-MM-dd\"))","metadata":{"execution":{"iopub.status.busy":"2024-01-17T08:42:22.965621Z","iopub.execute_input":"2024-01-17T08:42:22.967260Z","iopub.status.idle":"2024-01-17T08:42:23.025865Z","shell.execute_reply.started":"2024-01-17T08:42:22.967220Z","shell.execute_reply":"2024-01-17T08:42:23.024392Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"#df_tele.show(4)","metadata":{"execution":{"iopub.status.busy":"2024-01-17T08:42:23.027306Z","iopub.execute_input":"2024-01-17T08:42:23.027754Z","iopub.status.idle":"2024-01-17T08:42:23.033625Z","shell.execute_reply.started":"2024-01-17T08:42:23.027713Z","shell.execute_reply":"2024-01-17T08:42:23.032287Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import to_date, dayofmonth, floor\nfrom pyspark.sql.types import IntegerType\nfrom math import ceil\n\n# Initialize a Spark session\nspark = SparkSession.builder.appName(\"WeekOfMonth\").getOrCreate()\n\n# Assuming you have a DataFrame named df_tele\n# Convert the \"eventdate\" column to date type\ndf_tele = df_tele.withColumn(\"eventdate\", to_date(df_tele[\"eventdate\"], \"yyyy-MM-dd\"))\n\n# Calculate the day of the month\ndf_tele = df_tele.withColumn(\"day_of_month\", dayofmonth(df_tele[\"eventdate\"]))\n\n# Define a UDF to calculate the week of the month\nfrom pyspark.sql.functions import udf\n\ndef week_of_month(dt):\n    \"\"\" Returns the week of the month for the specified date.\"\"\"\n    dom = dt.day\n    adjusted_dom = dom + (dt.weekday() + 1)  # Adjusted for 1-based indexing\n    return int(ceil(adjusted_dom / 7.0))\n\n# Register the UDF\nweek_udf = udf(week_of_month, IntegerType())\n\n# Calculate the week of the month\ndf_tele = df_tele.withColumn(\"week_of_month\", week_udf(df_tele[\"eventdate\"]))\n\n# Show the resulting DataFrame\ndf_tele.show(3)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-01-17T08:42:23.042236Z","iopub.execute_input":"2024-01-17T08:42:23.042989Z","iopub.status.idle":"2024-01-17T08:42:28.266446Z","shell.execute_reply.started":"2024-01-17T08:42:23.042944Z","shell.execute_reply":"2024-01-17T08:42:28.263080Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"24/01/17 08:42:23 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n[Stage 9:=======================================>                   (2 + 1) / 3]\r","output_type":"stream"},{"name":"stdout","text":"+----------+-----------------+-----------+------------+----------+--------------+----------+------------+-------------+\n|Unnamed: 0|        vehicleId|gpsLatitude|gpsLongitude| eventdate|odometer_count|  District|day_of_month|week_of_month|\n+----------+-----------------+-----------+------------+----------+--------------+----------+------------+-------------+\n|         0|MAT422001N2F12927| 28.1071456|  76.8211136|2023-01-01|             3|     Alwar|           1|            2|\n|         1|MAT422001N2F13091| 18.2318608|   78.384608|2023-01-01|             2| Kamareddy|           1|            2|\n|         2|MAT422001N2F13447| 11.0975896|  77.1377472|2023-01-01|            13|Coimbatore|           1|            2|\n+----------+-----------------+-----------+------------+----------+--------------+----------+------------+-------------+\nonly showing top 3 rows\n\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"}]},{"cell_type":"code","source":"df_tele.select('day_of_month','week_of_month').show(12)","metadata":{"execution":{"iopub.status.busy":"2024-01-17T08:42:28.268201Z","iopub.execute_input":"2024-01-17T08:42:28.268635Z","iopub.status.idle":"2024-01-17T08:42:29.480055Z","shell.execute_reply.started":"2024-01-17T08:42:28.268595Z","shell.execute_reply":"2024-01-17T08:42:29.478846Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"+------------+-------------+\n|day_of_month|week_of_month|\n+------------+-------------+\n|           1|            2|\n|           1|            2|\n|           1|            2|\n|           1|            2|\n|           1|            2|\n|           1|            2|\n|           1|            2|\n|           1|            2|\n|           1|            2|\n|           1|            2|\n|           1|            2|\n|           1|            2|\n+------------+-------------+\nonly showing top 12 rows\n\n","output_type":"stream"}]},{"cell_type":"code","source":"# Select the distinct values of the \"week_of_month\" column\ndistinct_days = df_tele.select(\"day_of_month\").distinct().orderBy(\"day_of_month\")\n\n# Show the distinct values\ndistinct_days.show()","metadata":{"execution":{"iopub.status.busy":"2024-01-17T08:42:29.481287Z","iopub.execute_input":"2024-01-17T08:42:29.481685Z","iopub.status.idle":"2024-01-17T08:42:41.031432Z","shell.execute_reply.started":"2024-01-17T08:42:29.481645Z","shell.execute_reply":"2024-01-17T08:42:41.030357Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stderr","text":"[Stage 12:===========================================>              (3 + 1) / 4]\r","output_type":"stream"},{"name":"stdout","text":"+------------+\n|day_of_month|\n+------------+\n|           1|\n|           2|\n|           3|\n|           4|\n|           5|\n|           6|\n|           7|\n|           8|\n|           9|\n|          10|\n|          11|\n|          12|\n|          13|\n|          14|\n|          15|\n|          16|\n|          17|\n|          18|\n|          19|\n|          20|\n+------------+\nonly showing top 20 rows\n\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"}]},{"cell_type":"code","source":"distinct_days = df_tele.select(\"day_of_month\").distinct().orderBy(\"day_of_month\").collect()\n\n# Print all the distinct days\nfor row in distinct_days:\n    print(row[\"day_of_month\"])","metadata":{"execution":{"iopub.status.busy":"2024-01-17T08:42:41.032600Z","iopub.execute_input":"2024-01-17T08:42:41.033275Z","iopub.status.idle":"2024-01-17T08:42:51.729697Z","shell.execute_reply.started":"2024-01-17T08:42:41.033229Z","shell.execute_reply":"2024-01-17T08:42:51.728548Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"name":"stdout","text":"1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n","output_type":"stream"}]},{"cell_type":"code","source":"#from pyspark.sql.functions import year, month, weekofyear, dayofmonth, dayofweek, dayofyear, hour, minute, second, unix_timestamp\n#from pyspark.sql import DataFrame\n\n#def add_datepart_spark(df: DataFrame, field_name: str, prefix=None, drop=True, time=False) -> DataFrame:\n    # Extract the date-related features using PySpark functions\n    #df = df.withColumn(\"Year\", year(field_name))\n    #df = df.withColumn(\"Month\", month(field_name))\n    #df = df.withColumn(\"Week\", weekofyear(field_name))\n    #df = df.withColumn(\"Day\", dayofmonth(field_name))\n    #df = df.withColumn(\"Dayofweek\", dayofweek(field_name))\n    #df = df.withColumn(\"Dayofyear\", dayofyear(field_name))\n    #df = df.withColumn(\"Is_month_end\", (dayofmonth(field_name) == 31).cast(\"int\"))\n    #df = df.withColumn(\"Is_month_start\", (dayofmonth(field_name) == 1).cast(\"int\"))\n    #df = df.withColumn(\"Is_quarter_end\", (month(field_name) == 3).cast(\"int\"))\n    #df = df.withColumn(\"Is_quarter_start\", (month(field_name) == 1).cast(\"int\"))\n    #df = df.withColumn(\"Is_year_end\", (month(field_name) == 12).cast(\"int\"))\n    #df = df.withColumn(\"Is_year_start\", (month(field_name) == 1).cast(\"int\"))\n\n    #if time:\n        #df = df.withColumn(\"Hour\", hour(field_name))\n        #df = df.withColumn(\"Minute\", minute(field_name))\n        #df = df.withColumn(\"Second\", second(field_name))\n\n    # Add the \"Elapsed\" column\n    #df = df.withColumn(\"Elapsed\", (unix_timestamp(field_name) * 1000).cast(\"long\"))\n\n    # Drop the original date column if drop is True\n    #if drop:\n        #df = df.drop(field_name)\n\n    #return df\n\n# Usage:\n#df_tele = add_datepart_spark(df_tele, df_tele[\"eventdate\"], prefix='event')\n","metadata":{"execution":{"iopub.status.busy":"2024-01-17T08:42:51.730902Z","iopub.execute_input":"2024-01-17T08:42:51.732734Z","iopub.status.idle":"2024-01-17T08:42:51.743034Z","shell.execute_reply.started":"2024-01-17T08:42:51.732692Z","shell.execute_reply":"2024-01-17T08:42:51.741941Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"df_tele.printSchema()","metadata":{"execution":{"iopub.status.busy":"2024-01-17T08:42:51.745130Z","iopub.execute_input":"2024-01-17T08:42:51.746274Z","iopub.status.idle":"2024-01-17T08:42:51.762525Z","shell.execute_reply.started":"2024-01-17T08:42:51.746232Z","shell.execute_reply":"2024-01-17T08:42:51.761359Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"root\n |-- Unnamed: 0: long (nullable = true)\n |-- vehicleId: string (nullable = true)\n |-- gpsLatitude: double (nullable = true)\n |-- gpsLongitude: double (nullable = true)\n |-- eventdate: date (nullable = true)\n |-- odometer_count: long (nullable = true)\n |-- District: string (nullable = true)\n |-- day_of_month: integer (nullable = true)\n |-- week_of_month: integer (nullable = true)\n\n","output_type":"stream"}]},{"cell_type":"code","source":"#df_tele.show(4)","metadata":{"execution":{"iopub.status.busy":"2024-01-17T08:42:51.763910Z","iopub.execute_input":"2024-01-17T08:42:51.765846Z","iopub.status.idle":"2024-01-17T08:42:51.770481Z","shell.execute_reply.started":"2024-01-17T08:42:51.765803Z","shell.execute_reply":"2024-01-17T08:42:51.769382Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"#df_tele.select(\"eventdate\",\"day_of_month\",\"week_of_month\",\"Dayofweek\",\"Dayofyear\").show(10)","metadata":{"execution":{"iopub.status.busy":"2024-01-17T08:42:51.772595Z","iopub.execute_input":"2024-01-17T08:42:51.773438Z","iopub.status.idle":"2024-01-17T08:42:51.781400Z","shell.execute_reply.started":"2024-01-17T08:42:51.773397Z","shell.execute_reply":"2024-01-17T08:42:51.780227Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"#import pyspark.sql.functions as F\n\n#def cal_mode(data, latitude, longitude):\n    # Combine latitude and longitude into a single column\n    #data = data.withColumn('combined', F.concat_ws(' - ', data[latitude], data[longitude]))\n\n    # Calculate mode using a custom function\n    #mode_udf = F.udf(lambda x: x.mode()[0] if len(x) > 0 else None)\n    #mod = data.groupby(['vehicleId', 'week_of_month', 'District']).agg(mode_udf(F.collect_list('combined')).alias('mode'))\n\n    # Split the combined column into 'Lat' and 'Long'\n    #mod = mod.withColumn('Lat', F.split(mod['mode'], ' - ')[0])\n    #mod = mod.withColumn('Long', F.split(mod['mode'], ' - ')[1])\n\n    # Drop the intermediate 'combined' column\n    #mod = mod.drop('mode')\n\n    #return mod\n\n# This is where you define the df_mode variable\n#df_mode = cal_mode(df_tele, 'gpsLatitude', 'gpsLongitude')\n","metadata":{"execution":{"iopub.status.busy":"2024-01-17T08:42:51.783695Z","iopub.execute_input":"2024-01-17T08:42:51.784589Z","iopub.status.idle":"2024-01-17T08:42:51.791073Z","shell.execute_reply.started":"2024-01-17T08:42:51.784499Z","shell.execute_reply":"2024-01-17T08:42:51.790059Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"#print(df_mode.count())\n#df_mode.show(3)","metadata":{"execution":{"iopub.status.busy":"2024-01-17T08:42:51.793148Z","iopub.execute_input":"2024-01-17T08:42:51.793883Z","iopub.status.idle":"2024-01-17T08:42:51.806613Z","shell.execute_reply.started":"2024-01-17T08:42:51.793842Z","shell.execute_reply":"2024-01-17T08:42:51.805296Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"import pyspark.sql.functions as F\n\ndef cal_mode(data, latitude, longitude):\n    # Combining latitude and longitude into a single column\n    data = data.withColumn('combined', F.concat_ws(' - ', data[latitude], data[longitude]))\n    # Calculating mode using PySpark's aggregation functions\n    mod = data.groupby(['vehicleId', 'week_of_month', 'District']) \\\n              .agg(F.expr('collect_list(combined) as combined_list')) \\\n              .withColumn('mode', F.expr('aggregate(combined_list, cast(null as string), (acc, x) -> IFNULL(acc, x), acc -> acc)'))\n    # Splitting the combined column into 'Lat' and 'Long'\n    mod = mod.withColumn('Lat', F.split(mod['mode'], ' - ')[0])\n    mod = mod.withColumn('Long', F.split(mod['mode'], ' - ')[1])\n    # Drop the intermediate columns\n    mod = mod.drop('combined', 'combined_list', 'mode')\n    return mod\n\n# This is where you define the df_mode variable\ndf_mode1 = cal_mode(df_tele, 'gpsLatitude', 'gpsLongitude')\n","metadata":{"execution":{"iopub.status.busy":"2024-01-17T08:42:51.808638Z","iopub.execute_input":"2024-01-17T08:42:51.809409Z","iopub.status.idle":"2024-01-17T08:42:52.195273Z","shell.execute_reply.started":"2024-01-17T08:42:51.809370Z","shell.execute_reply":"2024-01-17T08:42:52.194073Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"df_mode1.count()","metadata":{"execution":{"iopub.status.busy":"2024-01-17T08:42:52.196773Z","iopub.execute_input":"2024-01-17T08:42:52.197237Z","iopub.status.idle":"2024-01-17T08:44:16.370565Z","shell.execute_reply.started":"2024-01-17T08:42:52.197192Z","shell.execute_reply":"2024-01-17T08:44:16.368885Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stderr","text":"24/01/17 08:43:41 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n24/01/17 08:43:42 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n24/01/17 08:43:45 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n24/01/17 08:43:46 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n24/01/17 08:43:52 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n24/01/17 08:43:52 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n24/01/17 08:43:53 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n24/01/17 08:43:53 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n24/01/17 08:43:55 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n24/01/17 08:43:55 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n24/01/17 08:44:12 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n24/01/17 08:44:12 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n24/01/17 08:44:12 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n24/01/17 08:44:12 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n24/01/17 08:44:12 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n24/01/17 08:44:12 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n24/01/17 08:44:12 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n24/01/17 08:44:12 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n                                                                                \r","output_type":"stream"},{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"4373456"},"metadata":{}}]},{"cell_type":"code","source":"#df_mode1.show()","metadata":{"execution":{"iopub.status.busy":"2024-01-17T08:44:16.373085Z","iopub.execute_input":"2024-01-17T08:44:16.373541Z","iopub.status.idle":"2024-01-17T08:44:16.381574Z","shell.execute_reply.started":"2024-01-17T08:44:16.373497Z","shell.execute_reply":"2024-01-17T08:44:16.379818Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"df_mode1.show(3)","metadata":{"execution":{"iopub.status.busy":"2024-01-17T08:44:16.385702Z","iopub.execute_input":"2024-01-17T08:44:16.387648Z","iopub.status.idle":"2024-01-17T08:45:52.715287Z","shell.execute_reply.started":"2024-01-17T08:44:16.387590Z","shell.execute_reply":"2024-01-17T08:45:52.713811Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stderr","text":"[Stage 31:>                                                         (0 + 1) / 1]\r","output_type":"stream"},{"name":"stdout","text":"+-----------------+-------------+----------+----------+----------+\n|        vehicleId|week_of_month|  District|       Lat|      Long|\n+-----------------+-------------+----------+----------+----------+\n|MAT422001N2F12927|            2|      Agra|27.1635584|77.8156032|\n|MAT422001N2F12927|            3|Chandigarh| 30.700688|76.7895808|\n|MAT422001N2F12927|            4|     Alwar|28.1101728|76.8164032|\n+-----------------+-------------+----------+----------+----------+\nonly showing top 3 rows\n\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"}]},{"cell_type":"code","source":"distinct_vid = df_tele.select(\"vehicleId\").distinct()\n\n# Show the distinct values\ndistinct_vid.count()","metadata":{"execution":{"iopub.status.busy":"2024-01-17T08:45:52.716630Z","iopub.execute_input":"2024-01-17T08:45:52.718323Z","iopub.status.idle":"2024-01-17T08:45:58.429556Z","shell.execute_reply.started":"2024-01-17T08:45:52.718251Z","shell.execute_reply":"2024-01-17T08:45:58.428422Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"377439"},"metadata":{}}]},{"cell_type":"code","source":"\ndistinct_vid_m = df_mode1.select(\"vehicleId\").distinct()\n\n# Show the distinct values\ndistinct_vid_m.count()","metadata":{"execution":{"iopub.status.busy":"2024-01-17T08:45:58.431534Z","iopub.execute_input":"2024-01-17T08:45:58.431951Z","iopub.status.idle":"2024-01-17T08:46:03.480427Z","shell.execute_reply.started":"2024-01-17T08:45:58.431915Z","shell.execute_reply":"2024-01-17T08:46:03.479251Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"377439"},"metadata":{}}]},{"cell_type":"code","source":"df_mode1 = df_mode1.withColumn('Lat', col('Lat').cast(\"float\"))\n\n# Cast 'Long' column to Float\ndf_mode1 = df_mode1.withColumn('Long', col('Long').cast(\"float\"))\n\n# Cast 'District' column to String (assuming it's not already a string)\ndf_mode1 = df_mode1.withColumn('District', col('District').cast(\"string\"))","metadata":{"execution":{"iopub.status.busy":"2024-01-17T08:46:03.482603Z","iopub.execute_input":"2024-01-17T08:46:03.483109Z","iopub.status.idle":"2024-01-17T08:46:03.535946Z","shell.execute_reply.started":"2024-01-17T08:46:03.483061Z","shell.execute_reply":"2024-01-17T08:46:03.534666Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"df_mode1.columns","metadata":{"execution":{"iopub.status.busy":"2024-01-17T08:46:03.537905Z","iopub.execute_input":"2024-01-17T08:46:03.540979Z","iopub.status.idle":"2024-01-17T08:46:03.552494Z","shell.execute_reply.started":"2024-01-17T08:46:03.540905Z","shell.execute_reply":"2024-01-17T08:46:03.551092Z"},"trusted":true},"execution_count":31,"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"['vehicleId', 'week_of_month', 'District', 'Lat', 'Long']"},"metadata":{}}]},{"cell_type":"code","source":"from pyspark.sql.functions import round\n\n# Assuming 'df_mode' is your PySpark DataFrame\n\n# Round 'Lat' column to two decimal places\ndf_mode1 = df_mode1.withColumn('Lat', round(df_mode1['Lat'], 2))\n\n# Round 'Long' column to two decimal places\ndf_mode1 = df_mode1.withColumn('Long', round(df_mode1['Long'], 2))\n","metadata":{"execution":{"iopub.status.busy":"2024-01-17T08:46:03.554795Z","iopub.execute_input":"2024-01-17T08:46:03.555626Z","iopub.status.idle":"2024-01-17T08:46:03.587528Z","shell.execute_reply.started":"2024-01-17T08:46:03.555585Z","shell.execute_reply":"2024-01-17T08:46:03.586427Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"from pyspark.sql import functions as F\n\n# Group by 'District' and count unique combinations of 'Lat' and 'Long'\nrou_off = df_mode1.groupBy('District', 'Lat', 'Long').agg(F.count(\"*\").alias(\"count\"))\n\n# Reset the index (not applicable to PySpark, but relevant in Pandas)\n# In PySpark, the result will be a DataFrame without an index\n\n# Print the number of rows and columns\nrow_count = rou_off.count()\ncolumn_count = len(rou_off.columns)\n\nprint(\"Number of rows:\", row_count)\nprint(\"Number of columns:\", column_count)\n","metadata":{"execution":{"iopub.status.busy":"2024-01-17T08:46:03.588657Z","iopub.execute_input":"2024-01-17T08:46:03.589051Z","iopub.status.idle":"2024-01-17T08:48:00.610483Z","shell.execute_reply.started":"2024-01-17T08:46:03.588999Z","shell.execute_reply":"2024-01-17T08:48:00.609119Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stderr","text":"[Stage 49:>                                                         (0 + 4) / 4]\r","output_type":"stream"},{"name":"stdout","text":"Number of rows: 390570\nNumber of columns: 4\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"}]},{"cell_type":"code","source":"rou_off.count()","metadata":{"execution":{"iopub.status.busy":"2024-01-17T08:48:00.612910Z","iopub.execute_input":"2024-01-17T08:48:00.613422Z","iopub.status.idle":"2024-01-17T08:49:51.161918Z","shell.execute_reply.started":"2024-01-17T08:48:00.613378Z","shell.execute_reply":"2024-01-17T08:49:51.160817Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"390664"},"metadata":{}}]},{"cell_type":"code","source":"rou_off.columns","metadata":{"execution":{"iopub.status.busy":"2024-01-17T08:49:51.163874Z","iopub.execute_input":"2024-01-17T08:49:51.164284Z","iopub.status.idle":"2024-01-17T08:49:51.178772Z","shell.execute_reply.started":"2024-01-17T08:49:51.164247Z","shell.execute_reply":"2024-01-17T08:49:51.177611Z"},"trusted":true},"execution_count":35,"outputs":[{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"['District', 'Lat', 'Long', 'count']"},"metadata":{}}]},{"cell_type":"code","source":"#df_region - has SSO\n#rou_off - has Distirct\n\n#df_region_renamed = df_region.withColumnRenamed(\"SSO\", \"District\")\n\nstate_sso_dis = spark.read.csv(\"/kaggle/input/district/District to SSO master.csv\",header=True,inferSchema=True)","metadata":{"execution":{"iopub.status.busy":"2024-01-17T08:49:51.182292Z","iopub.execute_input":"2024-01-17T08:49:51.183344Z","iopub.status.idle":"2024-01-17T08:49:51.649702Z","shell.execute_reply.started":"2024-01-17T08:49:51.183303Z","shell.execute_reply":"2024-01-17T08:49:51.648460Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"df_region.columns","metadata":{"execution":{"iopub.status.busy":"2024-01-17T08:49:51.651076Z","iopub.execute_input":"2024-01-17T08:49:51.651509Z","iopub.status.idle":"2024-01-17T08:49:51.663193Z","shell.execute_reply.started":"2024-01-17T08:49:51.651467Z","shell.execute_reply":"2024-01-17T08:49:51.661868Z"},"trusted":true},"execution_count":37,"outputs":[{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"['Division', 'Lattitude', 'Longitude', 'SSO', 'State']"},"metadata":{}}]},{"cell_type":"code","source":"state_sso_dis.columns","metadata":{"execution":{"iopub.status.busy":"2024-01-17T08:49:51.665577Z","iopub.execute_input":"2024-01-17T08:49:51.666396Z","iopub.status.idle":"2024-01-17T08:49:51.674740Z","shell.execute_reply.started":"2024-01-17T08:49:51.666353Z","shell.execute_reply":"2024-01-17T08:49:51.673510Z"},"trusted":true},"execution_count":38,"outputs":[{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"['District', 'vehicleId', 'State', 'SSO']"},"metadata":{}}]},{"cell_type":"code","source":"distinct_sso = state_sso_dis.select(\"SSO\").distinct()","metadata":{"execution":{"iopub.status.busy":"2024-01-17T08:49:51.676236Z","iopub.execute_input":"2024-01-17T08:49:51.677097Z","iopub.status.idle":"2024-01-17T08:49:51.700208Z","shell.execute_reply.started":"2024-01-17T08:49:51.677066Z","shell.execute_reply":"2024-01-17T08:49:51.699140Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"distinct_sso.show(32)","metadata":{"execution":{"iopub.status.busy":"2024-01-17T08:49:51.701499Z","iopub.execute_input":"2024-01-17T08:49:51.704293Z","iopub.status.idle":"2024-01-17T08:49:52.025270Z","shell.execute_reply.started":"2024-01-17T08:49:51.704257Z","shell.execute_reply":"2024-01-17T08:49:52.024060Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stdout","text":"+-----------+\n|        SSO|\n+-----------+\n|  Bangalore|\n|    Udaipur|\n|Bhubaneswar|\n|      Jammu|\n|      Hubli|\n|      Patna|\n|    Lucknow|\n|    Chennai|\n|     Mumbai|\n|  Ahmedabad|\n|    Kolkata|\n|  Allahabad|\n|      Salem|\n|   Lucknow |\n|   Dehradun|\n|    Gurgaon|\n|       Pune|\n|      Delhi|\n|        JSR|\n|       #N/A|\n|     Raipur|\n| Chandigarh|\n|     Nagpur|\n|     Bhopal|\n|     Cochin|\n|   Siliguri|\n|  Hyderabad|\n|     Rajkot|\n| Vijayawada|\n|   Guwahati|\n|     Jaipur|\n+-----------+\n\n","output_type":"stream"}]},{"cell_type":"code","source":"distinct_sso_region = df_region.select(\"SSO\").distinct()","metadata":{"execution":{"iopub.status.busy":"2024-01-17T08:49:52.037521Z","iopub.execute_input":"2024-01-17T08:49:52.038412Z","iopub.status.idle":"2024-01-17T08:49:52.061086Z","shell.execute_reply.started":"2024-01-17T08:49:52.038364Z","shell.execute_reply":"2024-01-17T08:49:52.059772Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"distinct_sso_region.count()","metadata":{"execution":{"iopub.status.busy":"2024-01-17T08:49:52.063260Z","iopub.execute_input":"2024-01-17T08:49:52.065361Z","iopub.status.idle":"2024-01-17T08:49:52.298351Z","shell.execute_reply.started":"2024-01-17T08:49:52.065294Z","shell.execute_reply":"2024-01-17T08:49:52.297077Z"},"trusted":true},"execution_count":42,"outputs":[{"execution_count":42,"output_type":"execute_result","data":{"text/plain":"32"},"metadata":{}}]},{"cell_type":"code","source":"distinct_sso.count()","metadata":{"execution":{"iopub.status.busy":"2024-01-17T08:49:52.302848Z","iopub.execute_input":"2024-01-17T08:49:52.304171Z","iopub.status.idle":"2024-01-17T08:49:52.499921Z","shell.execute_reply.started":"2024-01-17T08:49:52.304135Z","shell.execute_reply":"2024-01-17T08:49:52.498773Z"},"trusted":true},"execution_count":43,"outputs":[{"execution_count":43,"output_type":"execute_result","data":{"text/plain":"31"},"metadata":{}}]},{"cell_type":"code","source":"state_sso_dis.show(4)","metadata":{"execution":{"iopub.status.busy":"2024-01-17T08:49:52.501886Z","iopub.execute_input":"2024-01-17T08:49:52.502333Z","iopub.status.idle":"2024-01-17T08:49:52.641282Z","shell.execute_reply.started":"2024-01-17T08:49:52.502293Z","shell.execute_reply":"2024-01-17T08:49:52.640119Z"},"trusted":true},"execution_count":44,"outputs":[{"name":"stdout","text":"+--------------------+---------+--------------+----------+\n|            District|vehicleId|         State|       SSO|\n+--------------------+---------+--------------+----------+\n|            Adilabad|        3|     Telangana| Hyderabad|\n|                Agra|       85| Uttar Pradesh|   Lucknow|\n|         Kadapa(YSR)|       40|Andhra Pradesh| Hyderabad|\n|Sri Potti Sriramu...|       32|Andhra Pradesh|Vijayawada|\n+--------------------+---------+--------------+----------+\nonly showing top 4 rows\n\n","output_type":"stream"}]},{"cell_type":"code","source":"state_sso_dis.count()","metadata":{"execution":{"iopub.status.busy":"2024-01-17T08:49:52.642513Z","iopub.execute_input":"2024-01-17T08:49:52.642886Z","iopub.status.idle":"2024-01-17T08:49:52.774067Z","shell.execute_reply.started":"2024-01-17T08:49:52.642850Z","shell.execute_reply":"2024-01-17T08:49:52.773067Z"},"trusted":true},"execution_count":45,"outputs":[{"execution_count":45,"output_type":"execute_result","data":{"text/plain":"635"},"metadata":{}}]},{"cell_type":"code","source":"#df_mode1 = rou_off.join(df_region_renamed, on='District', how='inner')\ndf_model = rou_off.join(state_sso_dis[['District','SSO','State']], on='District', how='inner')\nprint(df_mode1.count())","metadata":{"execution":{"iopub.status.busy":"2024-01-17T08:49:52.775152Z","iopub.execute_input":"2024-01-17T08:49:52.776708Z","iopub.status.idle":"2024-01-17T08:51:13.227808Z","shell.execute_reply.started":"2024-01-17T08:49:52.776646Z","shell.execute_reply":"2024-01-17T08:51:13.226650Z"},"trusted":true},"execution_count":46,"outputs":[{"name":"stderr","text":"24/01/17 08:50:41 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n24/01/17 08:50:42 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n24/01/17 08:50:49 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n24/01/17 08:50:50 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n24/01/17 08:50:51 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n24/01/17 08:50:51 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n24/01/17 08:50:51 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n24/01/17 08:50:51 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n24/01/17 08:50:53 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n24/01/17 08:50:54 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n24/01/17 08:51:09 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n24/01/17 08:51:09 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n24/01/17 08:51:09 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n24/01/17 08:51:09 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n24/01/17 08:51:09 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n24/01/17 08:51:09 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n24/01/17 08:51:10 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n24/01/17 08:51:10 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n[Stage 87:=============================>                            (2 + 2) / 4]\r","output_type":"stream"},{"name":"stdout","text":"4373456\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Show the resulting DataFrame\ndf_model.show()\n\n# Print the number of rows and columns\nprint(\"Number of rows:\", df_mode1.count())\nprint(\"Number of columns:\", len(df_mode1.columns))","metadata":{"execution":{"iopub.status.busy":"2024-01-17T08:51:13.229008Z","iopub.execute_input":"2024-01-17T08:51:13.235428Z","iopub.status.idle":"2024-01-17T08:54:24.560586Z","shell.execute_reply.started":"2024-01-17T08:51:13.235387Z","shell.execute_reply":"2024-01-17T08:54:24.559189Z"},"trusted":true},"execution_count":47,"outputs":[{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"name":"stdout","text":"+--------------------+-----+-----+-----+-----------+--------------------+\n|            District|  Lat| Long|count|        SSO|               State|\n+--------------------+-----+-----+-----+-----------+--------------------+\n|              Jaipur|26.82| 75.5|   61|     Jaipur|           Rajasthan|\n|               Haora|22.59|88.22| 1010|    Kolkata|         West Bengal|\n|          Srikakulam|18.38| 84.0|  217| Vijayawada|      Andhra Pradesh|\n|                Durg|21.21|81.29|  264|     Raipur|         Chhatisgarh|\n|               Nadia|23.72| 88.3|   12|    Kolkata|         West Bengal|\n|              Rohtak|28.89|76.56|  110|    Gurgaon|             Haryana|\n|               Morbi|23.07|70.84|   51|     Rajkot|             Gujarat|\n|               Patan|23.78|71.62|    5|  Ahmedabad|             Gujarat|\n|        Kanpur Nagar|26.45|80.24|  195|    Lucknow|       Uttar Pradesh|\n|Dadra & Nagar Haveli|20.31|73.06|   15|  Ahmedabad|Dadra & Nagar Haveli|\n|          Jalpaiguri|26.54| 88.8|  226|   Siliguri|         West Bengal|\n|              Bokaro|23.63|86.29|   83|        JSR|           Jharkhand|\n|              Palwal|28.16|77.32|   77|    Gurgaon|             Haryana|\n|            Hamirpur| 25.6|80.07|  333| Chandigarh|    Himachal Pradesh|\n|     Mumbai Suburban|19.07|72.87|  226|     Mumbai|         Maharashtra|\n|             Palghar|19.72|72.93|  151|     Mumbai|         Maharashtra|\n|              Anugul| 20.8|85.26|  271|Bhubaneswar|              Orissa|\n|         Bulandshahr|28.47|77.67|  378|      Delhi|       Uttar Pradesh|\n|          Jhunjhunun|28.26|75.67|   73|     Jaipur|           Rajasthan|\n|             Kachchh|23.74| 69.3|   28|     Rajkot|             Gujarat|\n+--------------------+-----+-----+-----+-----------+--------------------+\nonly showing top 20 rows\n\n","output_type":"stream"},{"name":"stderr","text":"24/01/17 08:53:54 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n24/01/17 08:53:55 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n24/01/17 08:54:01 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n24/01/17 08:54:02 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n24/01/17 08:54:03 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n24/01/17 08:54:03 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n24/01/17 08:54:04 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n24/01/17 08:54:04 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n24/01/17 08:54:05 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n24/01/17 08:54:06 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n24/01/17 08:54:21 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n24/01/17 08:54:21 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n24/01/17 08:54:21 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n24/01/17 08:54:21 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n24/01/17 08:54:21 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n24/01/17 08:54:21 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n24/01/17 08:54:21 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n24/01/17 08:54:21 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n","output_type":"stream"},{"name":"stdout","text":"Number of rows: 4373456\nNumber of columns: 5\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"}]},{"cell_type":"code","source":"import pyspark.sql.functions as F\n\ndef filter_df(df, sso_col, sso_list):\n    '''\n    Filters the DataFrame to only the rows where the sso column is in the sso_list\n    '''\n\n    # Creating a filter expression\n    filter_expr = F.col(sso_col).isin(sso_list)\n\n    # Filtering the DataFrame\n    df_filtered = df.filter(filter_expr)\n\n    return df_filtered\n","metadata":{"execution":{"iopub.status.busy":"2024-01-17T08:54:24.561816Z","iopub.execute_input":"2024-01-17T08:54:24.562227Z","iopub.status.idle":"2024-01-17T08:54:24.571289Z","shell.execute_reply.started":"2024-01-17T08:54:24.562190Z","shell.execute_reply":"2024-01-17T08:54:24.570108Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"sso_list = [row.SSO for row in state_sso_dis.select(\"SSO\").collect()]","metadata":{"execution":{"iopub.status.busy":"2024-01-17T08:54:24.573406Z","iopub.execute_input":"2024-01-17T08:54:24.574292Z","iopub.status.idle":"2024-01-17T08:54:24.738057Z","shell.execute_reply.started":"2024-01-17T08:54:24.574247Z","shell.execute_reply":"2024-01-17T08:54:24.736780Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Filtering the DataFrame\nfinal_df = filter_df(df_model, 'SSO', sso_list)","metadata":{"execution":{"iopub.status.busy":"2024-01-17T08:54:24.740200Z","iopub.execute_input":"2024-01-17T08:54:24.740986Z","iopub.status.idle":"2024-01-17T08:54:25.894195Z","shell.execute_reply.started":"2024-01-17T08:54:24.740945Z","shell.execute_reply":"2024-01-17T08:54:25.891907Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"final_df.show()","metadata":{"execution":{"iopub.status.busy":"2024-01-17T08:54:25.896070Z","iopub.execute_input":"2024-01-17T08:54:25.896541Z","iopub.status.idle":"2024-01-17T08:56:16.447617Z","shell.execute_reply.started":"2024-01-17T08:54:25.896497Z","shell.execute_reply":"2024-01-17T08:56:16.446327Z"},"trusted":true},"execution_count":51,"outputs":[{"name":"stderr","text":"[Stage 108:===============================================>         (5 + 1) / 6]\r","output_type":"stream"},{"name":"stdout","text":"+--------------------+-----+-----+-----+-----------+--------------------+\n|            District|  Lat| Long|count|        SSO|               State|\n+--------------------+-----+-----+-----+-----------+--------------------+\n|              Jaipur|26.82| 75.5|   62|     Jaipur|           Rajasthan|\n|               Patna|25.57|85.24|  337|      Patna|               Bihar|\n|               Haora|22.59|88.22| 1004|    Kolkata|         West Bengal|\n|          Srikakulam|18.38| 84.0|  219| Vijayawada|      Andhra Pradesh|\n|                Durg|21.21|81.29|  265|     Raipur|         Chhatisgarh|\n|               Nadia|23.72| 88.3|   12|    Kolkata|         West Bengal|\n|              Rohtak|28.89|76.56|  110|    Gurgaon|             Haryana|\n|               Morbi|23.07|70.84|   51|     Rajkot|             Gujarat|\n|               Patan|23.78|71.62|    5|  Ahmedabad|             Gujarat|\n|        Kanpur Nagar|26.45|80.24|  196|    Lucknow|       Uttar Pradesh|\n|Dadra & Nagar Haveli|20.31|73.06|   15|  Ahmedabad|Dadra & Nagar Haveli|\n|          Jalpaiguri|26.54| 88.8|  225|   Siliguri|         West Bengal|\n|              Bokaro|23.63|86.29|   84|        JSR|           Jharkhand|\n|              Palwal|28.16|77.32|   79|    Gurgaon|             Haryana|\n|            Hamirpur| 25.6|80.07|  331| Chandigarh|    Himachal Pradesh|\n|     Mumbai Suburban|19.07|72.87|  226|     Mumbai|         Maharashtra|\n|             Palghar|19.72|72.93|  150|     Mumbai|         Maharashtra|\n|              Anugul| 20.8|85.26|  268|Bhubaneswar|              Orissa|\n|         Bulandshahr|28.47|77.67|  378|      Delhi|       Uttar Pradesh|\n|            Bilaspur|22.02|82.12|   10|     Raipur|    Himachal Pradesh|\n+--------------------+-----+-----+-----+-----------+--------------------+\nonly showing top 20 rows\n\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"}]},{"cell_type":"code","source":"#del_cha.count()\n","metadata":{"execution":{"iopub.status.busy":"2024-01-17T08:56:16.449202Z","iopub.execute_input":"2024-01-17T08:56:16.450706Z","iopub.status.idle":"2024-01-17T08:56:16.455846Z","shell.execute_reply.started":"2024-01-17T08:56:16.450656Z","shell.execute_reply":"2024-01-17T08:56:16.454757Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"final_df.count()","metadata":{"execution":{"iopub.status.busy":"2024-01-17T08:56:16.457054Z","iopub.execute_input":"2024-01-17T08:56:16.457451Z","iopub.status.idle":"2024-01-17T08:58:08.863195Z","shell.execute_reply.started":"2024-01-17T08:56:16.457413Z","shell.execute_reply":"2024-01-17T08:58:08.862073Z"},"trusted":true},"execution_count":53,"outputs":[{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"execution_count":53,"output_type":"execute_result","data":{"text/plain":"380514"},"metadata":{}}]},{"cell_type":"code","source":"## delhi\nwork_reg = filter_df(df_region,'SSO',['Delhi', 'Lucknow'])\n","metadata":{"execution":{"iopub.status.busy":"2024-01-17T08:58:08.864473Z","iopub.execute_input":"2024-01-17T08:58:08.864870Z","iopub.status.idle":"2024-01-17T08:58:08.932002Z","shell.execute_reply.started":"2024-01-17T08:58:08.864833Z","shell.execute_reply":"2024-01-17T08:58:08.930601Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"# PAN-INDIA \nwork_reg = filter_df(df_region,'SSO',sso_list)","metadata":{"execution":{"iopub.status.busy":"2024-01-17T08:58:08.933406Z","iopub.execute_input":"2024-01-17T08:58:08.933887Z","iopub.status.idle":"2024-01-17T08:58:09.771930Z","shell.execute_reply.started":"2024-01-17T08:58:08.933814Z","shell.execute_reply":"2024-01-17T08:58:09.770356Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"work_reg.show(4)","metadata":{"execution":{"iopub.status.busy":"2024-01-17T08:58:09.773691Z","iopub.execute_input":"2024-01-17T08:58:09.774404Z","iopub.status.idle":"2024-01-17T08:58:09.960761Z","shell.execute_reply.started":"2024-01-17T08:58:09.774361Z","shell.execute_reply":"2024-01-17T08:58:09.959724Z"},"trusted":true},"execution_count":56,"outputs":[{"name":"stdout","text":"+--------------------+---------+----------+------+----------+\n|            Division|Lattitude| Longitude|   SSO|     State|\n+--------------------+---------+----------+------+----------+\n|100B520-Sv&Pa-Nag...|8.1908371|77.4192042| Salem|Tamil Nadu|\n|1001650-Sv&Pa-Nag...| 8.198264| 77.431443| Salem|Tamil Nadu|\n|2088645-Sv&Pa-Ney...|   8.3681|  77.08208|Cochin|    Kerala|\n|2089374-Sv&Pa-Val...| 8.370961| 77.601832| Salem|Tamil Nadu|\n+--------------------+---------+----------+------+----------+\nonly showing top 4 rows\n\n","output_type":"stream"}]},{"cell_type":"code","source":"work_reg.count()","metadata":{"execution":{"iopub.status.busy":"2024-01-17T08:58:09.961816Z","iopub.execute_input":"2024-01-17T08:58:09.962184Z","iopub.status.idle":"2024-01-17T08:58:10.255815Z","shell.execute_reply.started":"2024-01-17T08:58:09.962147Z","shell.execute_reply":"2024-01-17T08:58:10.254664Z"},"trusted":true},"execution_count":57,"outputs":[{"execution_count":57,"output_type":"execute_result","data":{"text/plain":"1332"},"metadata":{}}]},{"cell_type":"code","source":"final_df.columns","metadata":{"execution":{"iopub.status.busy":"2024-01-17T08:58:10.257009Z","iopub.execute_input":"2024-01-17T08:58:10.257521Z","iopub.status.idle":"2024-01-17T08:58:10.273984Z","shell.execute_reply.started":"2024-01-17T08:58:10.257481Z","shell.execute_reply":"2024-01-17T08:58:10.272470Z"},"trusted":true},"execution_count":58,"outputs":[{"execution_count":58,"output_type":"execute_result","data":{"text/plain":"['District', 'Lat', 'Long', 'count', 'SSO', 'State']"},"metadata":{}}]},{"cell_type":"code","source":"work_reg.columns","metadata":{"execution":{"iopub.status.busy":"2024-01-17T08:58:10.275861Z","iopub.execute_input":"2024-01-17T08:58:10.281332Z","iopub.status.idle":"2024-01-17T08:58:10.348328Z","shell.execute_reply.started":"2024-01-17T08:58:10.281284Z","shell.execute_reply":"2024-01-17T08:58:10.347264Z"},"trusted":true},"execution_count":59,"outputs":[{"execution_count":59,"output_type":"execute_result","data":{"text/plain":"['Division', 'Lattitude', 'Longitude', 'SSO', 'State']"},"metadata":{}}]},{"cell_type":"code","source":"#del_cha.count()","metadata":{"execution":{"iopub.status.busy":"2024-01-17T08:58:10.352924Z","iopub.execute_input":"2024-01-17T08:58:10.353342Z","iopub.status.idle":"2024-01-17T08:58:10.360691Z","shell.execute_reply.started":"2024-01-17T08:58:10.353306Z","shell.execute_reply":"2024-01-17T08:58:10.359538Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"code","source":"#del_cha_pd = del_cha.toPandas()\n#work_reg_pd = work_reg.toPandas()","metadata":{"execution":{"iopub.status.busy":"2024-01-17T08:58:10.363231Z","iopub.execute_input":"2024-01-17T08:58:10.364715Z","iopub.status.idle":"2024-01-17T08:58:10.374152Z","shell.execute_reply.started":"2024-01-17T08:58:10.364674Z","shell.execute_reply":"2024-01-17T08:58:10.373180Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"code","source":"final_df.describe().show()\n","metadata":{"execution":{"iopub.status.busy":"2024-01-17T08:58:10.376221Z","iopub.execute_input":"2024-01-17T08:58:10.377080Z","iopub.status.idle":"2024-01-17T09:00:03.430475Z","shell.execute_reply.started":"2024-01-17T08:58:10.377041Z","shell.execute_reply":"2024-01-17T09:00:03.427741Z"},"trusted":true},"execution_count":62,"outputs":[{"name":"stderr","text":"24/01/17 08:58:10 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n[Stage 133:==========================================>              (3 + 1) / 4]\r","output_type":"stream"},{"name":"stdout","text":"+-------+--------+------------------+-----------------+------------------+----------+--------------------+\n|summary|District|               Lat|             Long|             count|       SSO|               State|\n+-------+--------+------------------+-----------------+------------------+----------+--------------------+\n|  count|  380499|            380499|           380499|            380499|    380499|              380499|\n|   mean|    NULL|22.572518665234348|79.60510274657055|11.108775581538978|      NULL|                NULL|\n| stddev|    NULL| 5.602468674327833|5.480422783076958| 50.12034638916672|      NULL|                NULL|\n|    min|Adilabad|              8.08|            68.54|                 1|      #N/A|Andaman & Nicobar...|\n|    max| Yadadri|             34.61|            97.03|             11872|Vijayawada|         West Bengal|\n+-------+--------+------------------+-----------------+------------------+----------+--------------------+\n\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"}]},{"cell_type":"code","source":"work_reg.describe().show()","metadata":{"execution":{"iopub.status.busy":"2024-01-17T09:00:03.432514Z","iopub.execute_input":"2024-01-17T09:00:03.433461Z","iopub.status.idle":"2024-01-17T09:00:04.323954Z","shell.execute_reply.started":"2024-01-17T09:00:03.433416Z","shell.execute_reply":"2024-01-17T09:00:04.322418Z"},"trusted":true},"execution_count":63,"outputs":[{"name":"stdout","text":"+-------+--------------------+------------------+--------------------+----------+--------------+\n|summary|            Division|         Lattitude|           Longitude|       SSO|         State|\n+-------+--------------------+------------------+--------------------+----------+--------------+\n|  count|                1332|              1330|                1330|      1332|          1331|\n|   mean|                NULL|22.539841702507598|    705050.736262136|      NULL|          NULL|\n| stddev|                NULL|  6.89309862403507|2.1611854444702957E7|      NULL|          NULL|\n|    min|1000030-Sv&Pa-Nag...|         10.043523|           20.288639| Ahmedabad|Andhra Pradesh|\n|    max|2089696-Sv&Pa-Bag...|          9.993807|           95.556614|Vijayawada|  West Bengal |\n+-------+--------------------+------------------+--------------------+----------+--------------+\n\n","output_type":"stream"}]},{"cell_type":"code","source":"final_df.printSchema()","metadata":{"execution":{"iopub.status.busy":"2024-01-17T09:00:04.325930Z","iopub.execute_input":"2024-01-17T09:00:04.326394Z","iopub.status.idle":"2024-01-17T09:00:04.334408Z","shell.execute_reply.started":"2024-01-17T09:00:04.326353Z","shell.execute_reply":"2024-01-17T09:00:04.333314Z"},"trusted":true},"execution_count":64,"outputs":[{"name":"stdout","text":"root\n |-- District: string (nullable = true)\n |-- Lat: float (nullable = true)\n |-- Long: float (nullable = true)\n |-- count: long (nullable = false)\n |-- SSO: string (nullable = true)\n |-- State: string (nullable = true)\n\n","output_type":"stream"}]},{"cell_type":"code","source":"work_reg.printSchema()","metadata":{"execution":{"iopub.status.busy":"2024-01-17T09:00:04.335652Z","iopub.execute_input":"2024-01-17T09:00:04.336068Z","iopub.status.idle":"2024-01-17T09:00:04.346841Z","shell.execute_reply.started":"2024-01-17T09:00:04.336013Z","shell.execute_reply":"2024-01-17T09:00:04.345536Z"},"trusted":true},"execution_count":65,"outputs":[{"name":"stdout","text":"root\n |-- Division: string (nullable = true)\n |-- Lattitude: string (nullable = true)\n |-- Longitude: string (nullable = true)\n |-- SSO: string (nullable = true)\n |-- State: string (nullable = true)\n\n","output_type":"stream"}]},{"cell_type":"code","source":"final_df_cleaned = final_df.na.drop(subset=[\"SSO\"]).filter(col(\"SSO\") != \"#N/A\")","metadata":{"execution":{"iopub.status.busy":"2024-01-17T09:00:04.351069Z","iopub.execute_input":"2024-01-17T09:00:04.351961Z","iopub.status.idle":"2024-01-17T09:00:04.392338Z","shell.execute_reply.started":"2024-01-17T09:00:04.351914Z","shell.execute_reply":"2024-01-17T09:00:04.391143Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"code","source":"work_reg_cleaned = work_reg.na.drop(subset=[\"SSO\"]).filter(col(\"SSO\") != \"#N/A\")","metadata":{"execution":{"iopub.status.busy":"2024-01-17T09:00:04.393537Z","iopub.execute_input":"2024-01-17T09:00:04.393933Z","iopub.status.idle":"2024-01-17T09:00:04.430224Z","shell.execute_reply.started":"2024-01-17T09:00:04.393893Z","shell.execute_reply":"2024-01-17T09:00:04.429052Z"},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"code","source":"#r = final_df_cleaned.select('SSO').distinct()","metadata":{"execution":{"iopub.status.busy":"2024-01-17T09:00:04.431389Z","iopub.execute_input":"2024-01-17T09:00:04.431772Z","iopub.status.idle":"2024-01-17T09:00:04.438184Z","shell.execute_reply.started":"2024-01-17T09:00:04.431735Z","shell.execute_reply":"2024-01-17T09:00:04.436860Z"},"trusted":true},"execution_count":68,"outputs":[]},{"cell_type":"code","source":"#r.show(30)","metadata":{"execution":{"iopub.status.busy":"2024-01-17T09:00:04.440501Z","iopub.execute_input":"2024-01-17T09:00:04.441599Z","iopub.status.idle":"2024-01-17T09:00:04.446798Z","shell.execute_reply.started":"2024-01-17T09:00:04.441557Z","shell.execute_reply":"2024-01-17T09:00:04.445637Z"},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"code","source":"from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.types import FloatType\nfrom pyspark.sql.window import Window\nfrom pyspark.sql import functions as F\n\n# Define Haversine distance UDF\n@udf(returnType=FloatType())\ndef haversine_distance(lat1, lon1, lat2, lon2):\n    from math import radians, sin, cos, sqrt, atan2\n\n    # Convert degrees to radians\n    lat1, lon1, lat2, lon2 = map(radians, [lat1, lon1, lat2, lon2])\n\n    # Haversine formula\n    dlon = lon2 - lon1\n    dlat = lat2 - lat1\n    a = sin(dlat / 2)**2 + cos(lat1) * cos(lat2) * sin(dlon / 2)**2\n    c = 2 * atan2(sqrt(a), sqrt(1 - a))\n    distance = 6371.0 * c  # Earth radius in kilometers\n\n    return distance\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-01-17T09:00:04.449016Z","iopub.execute_input":"2024-01-17T09:00:04.449987Z","iopub.status.idle":"2024-01-17T09:00:04.461068Z","shell.execute_reply.started":"2024-01-17T09:00:04.449948Z","shell.execute_reply":"2024-01-17T09:00:04.459948Z"},"trusted":true},"execution_count":70,"outputs":[]},{"cell_type":"code","source":"final_df_cleaned.printSchema()","metadata":{"execution":{"iopub.status.busy":"2024-01-17T09:00:04.463145Z","iopub.execute_input":"2024-01-17T09:00:04.463834Z","iopub.status.idle":"2024-01-17T09:00:04.475634Z","shell.execute_reply.started":"2024-01-17T09:00:04.463797Z","shell.execute_reply":"2024-01-17T09:00:04.474317Z"},"trusted":true},"execution_count":71,"outputs":[{"name":"stdout","text":"root\n |-- District: string (nullable = true)\n |-- Lat: float (nullable = true)\n |-- Long: float (nullable = true)\n |-- count: long (nullable = false)\n |-- SSO: string (nullable = true)\n |-- State: string (nullable = true)\n\n","output_type":"stream"}]},{"cell_type":"code","source":"work_reg_cleaned.printSchema","metadata":{"execution":{"iopub.status.busy":"2024-01-17T09:00:04.477910Z","iopub.execute_input":"2024-01-17T09:00:04.478935Z","iopub.status.idle":"2024-01-17T09:00:04.495284Z","shell.execute_reply.started":"2024-01-17T09:00:04.478896Z","shell.execute_reply":"2024-01-17T09:00:04.493560Z"},"trusted":true},"execution_count":72,"outputs":[{"execution_count":72,"output_type":"execute_result","data":{"text/plain":"<bound method DataFrame.printSchema of DataFrame[Division: string, Lattitude: string, Longitude: string, SSO: string, State: string]>"},"metadata":{}}]},{"cell_type":"code","source":"min_max_values = final_df_cleaned.agg(min(\"Lat\").alias(\"min_latitude\"),\n                        max(\"Lat\").alias(\"max_latitude\"),\n                        min(\"Long\").alias(\"min_longitude\"),\n                        max(\"Long\").alias(\"max_longitude\"))\n\n# Show the result\nmin_max_values.show()","metadata":{"execution":{"iopub.status.busy":"2024-01-17T09:00:04.502082Z","iopub.execute_input":"2024-01-17T09:00:04.502570Z","iopub.status.idle":"2024-01-17T09:01:56.549082Z","shell.execute_reply.started":"2024-01-17T09:00:04.502529Z","shell.execute_reply":"2024-01-17T09:01:56.548018Z"},"trusted":true},"execution_count":73,"outputs":[{"name":"stderr","text":"[Stage 147:==============>                                          (1 + 3) / 4]\r","output_type":"stream"},{"name":"stdout","text":"+------------+------------+-------------+-------------+\n|min_latitude|max_latitude|min_longitude|max_longitude|\n+------------+------------+-------------+-------------+\n|        8.08|       34.61|        68.54|        97.03|\n+------------+------------+-------------+-------------+\n\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"}]},{"cell_type":"code","source":"min_max_values = work_reg_cleaned.agg(min(\"Lattitude\").alias(\"min_latitude\"),\n                        max(\"Lattitude\").alias(\"max_latitude\"),\n                        min(\"Longitude\").alias(\"min_longitude\"),\n                        max(\"Longitude\").alias(\"max_longitude\"))\n\n# Show the result\nmin_max_values.show()","metadata":{"execution":{"iopub.status.busy":"2024-01-17T09:01:56.550275Z","iopub.execute_input":"2024-01-17T09:01:56.550908Z","iopub.status.idle":"2024-01-17T09:01:57.070980Z","shell.execute_reply.started":"2024-01-17T09:01:56.550868Z","shell.execute_reply":"2024-01-17T09:01:57.069491Z"},"trusted":true},"execution_count":74,"outputs":[{"name":"stdout","text":"+------------+------------+-------------+-------------+\n|min_latitude|max_latitude|min_longitude|max_longitude|\n+------------+------------+-------------+-------------+\n|   10.043523|    9.993807|    20.288639|    95.556614|\n+------------+------------+-------------+-------------+\n\n","output_type":"stream"}]},{"cell_type":"code","source":"work_reg_cleaned.columns","metadata":{"execution":{"iopub.status.busy":"2024-01-17T09:01:57.072692Z","iopub.execute_input":"2024-01-17T09:01:57.073276Z","iopub.status.idle":"2024-01-17T09:01:57.084376Z","shell.execute_reply.started":"2024-01-17T09:01:57.073234Z","shell.execute_reply":"2024-01-17T09:01:57.083087Z"},"trusted":true},"execution_count":75,"outputs":[{"execution_count":75,"output_type":"execute_result","data":{"text/plain":"['Division', 'Lattitude', 'Longitude', 'SSO', 'State']"},"metadata":{}}]},{"cell_type":"code","source":"from pyspark.sql.functions import col, udf\nfrom pyspark.sql.types import DoubleType\nfrom math import radians, sin, cos, sqrt, atan2\n\n# Define the Haversine distance function\ndef haversine(lat1, lon1, lat2, lon2):\n    lat1, lon1, lat2, lon2 = map(radians, [lat1, lon1, lat2, lon2])\n\n    dlat = lat2 - lat1\n    dlon = lon2 - lon1\n\n    a = sin(dlat / 2) ** 2 + cos(lat1) * cos(lat2) * sin(dlon / 2) ** 2\n    c = 2 * atan2(sqrt(a), sqrt(1 - a))\n\n    return 6371.0 * c  # Radius of Earth in kilometers\n\n# Register the Haversine function as a UDF\nhaversine_udf = udf(haversine, DoubleType())\n","metadata":{"execution":{"iopub.status.busy":"2024-01-17T09:01:57.087756Z","iopub.execute_input":"2024-01-17T09:01:57.093725Z","iopub.status.idle":"2024-01-17T09:01:57.108512Z","shell.execute_reply.started":"2024-01-17T09:01:57.093671Z","shell.execute_reply":"2024-01-17T09:01:57.107311Z"},"trusted":true},"execution_count":76,"outputs":[]},{"cell_type":"code","source":"# Cross-join the two DataFrames\ncombined_df = final_df_cleaned.crossJoin(work_reg_cleaned)\n\n# Calculate the distance for each combination of coordinates\ncombined_df = combined_df.withColumn(\n    \"distance\",\n    haversine_udf(\n        col(\"Lat\"), col(\"Long\"), col(\"Lattitude\"), col(\"Longitude\")\n    )\n)\n\n# Find the minimum distance for each row in final_df_cleaned\nfrom pyspark.sql.window import Window\nfrom pyspark.sql import functions as F\n\nwindow_spec = Window.partitionBy(\"District\").orderBy(\"distance\")\n\nfinal_df_with_min_distance = combined_df.withColumn(\n    \"min_distance\",\n    F.first(\"distance\").over(window_spec)\n)\n\n# Select the relevant columns and drop duplicates\nfinal_df_with_min_distance = final_df_with_min_distance.select(\n    \"District\", \"Lat\", \"Long\", \"count\", \"SSO\", \"State\", \"min_distance\"\n).distinct()\n\n# Show the resulting DataFrame with the minimum distances\nfinal_df_with_min_distance.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-01-17T09:01:57.110966Z","iopub.execute_input":"2024-01-17T09:01:57.111869Z","iopub.status.idle":"2024-01-17T09:01:58.203618Z","shell.execute_reply.started":"2024-01-17T09:01:57.111828Z","shell.execute_reply":"2024-01-17T09:01:58.201640Z"},"trusted":true},"execution_count":77,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[77], line 24\u001b[0m\n\u001b[1;32m     18\u001b[0m final_df_with_min_distance \u001b[38;5;241m=\u001b[39m combined_df\u001b[38;5;241m.\u001b[39mwithColumn(\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin_distance\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     20\u001b[0m     F\u001b[38;5;241m.\u001b[39mfirst(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdistance\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mover(window_spec)\n\u001b[1;32m     21\u001b[0m )\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Select the relevant columns and drop duplicates\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m final_df_with_min_distance \u001b[38;5;241m=\u001b[39m \u001b[43mfinal_df_with_min_distance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDistrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mLat\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mLong\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcount\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSSO\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mState\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmin_distance\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     26\u001b[0m \u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdistinct()\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Show the resulting DataFrame with the minimum distances\u001b[39;00m\n\u001b[1;32m     29\u001b[0m final_df_with_min_distance\u001b[38;5;241m.\u001b[39mshow()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pyspark/sql/dataframe.py:3223\u001b[0m, in \u001b[0;36mDataFrame.select\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   3178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mselect\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mcols: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumnOrName\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataFrame\u001b[39m\u001b[38;5;124m\"\u001b[39m:  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   3179\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Projects a set of expressions and returns a new :class:`DataFrame`.\u001b[39;00m\n\u001b[1;32m   3180\u001b[0m \n\u001b[1;32m   3181\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3221\u001b[0m \u001b[38;5;124;03m    +-----+---+\u001b[39;00m\n\u001b[1;32m   3222\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 3223\u001b[0m     jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jcols\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcols\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(jdf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n","\u001b[0;31mAnalysisException\u001b[0m: [AMBIGUOUS_REFERENCE] Reference `SSO` is ambiguous, could be: [`SSO`, `SSO`]."],"ename":"AnalysisException","evalue":"[AMBIGUOUS_REFERENCE] Reference `SSO` is ambiguous, could be: [`SSO`, `SSO`].","output_type":"error"}]},{"cell_type":"code","source":"valid_latitude_range = (-90.0, 90.0)\nvalid_longitude_range = (-180.0, 180.0)\n\n# Filter and clean work_reg_df\nwork_reg_cleaned = work_reg_cleaned.withColumn(\"Latitude\",when(col(\"Lattitude\").between(valid_latitude_range[0], valid_latitude_range[1]), col(\"Lattitude\")).otherwise(None)).withColumn(\"Longitude\",when(col(\"Longitude\").between(valid_longitude_range[0], valid_longitude_range[1]), col(\"Longitude\")).otherwise(None)).na.drop()\n\n# Filter and clean del_cha_df\nfinal_df_cleaned = final_df_cleaned.withColumn(\"Lat\",when(col(\"Lat\").between(valid_latitude_range[0], valid_latitude_range[1]), col(\"Lat\")).otherwise(None)).withColumn(\"Long\",when(col(\"Long\").between(valid_longitude_range[0], valid_longitude_range[1]), col(\"Long\")).otherwise(None)).na.drop()","metadata":{"execution":{"iopub.status.busy":"2024-01-17T09:01:58.204571Z","iopub.status.idle":"2024-01-17T09:01:58.205370Z","shell.execute_reply.started":"2024-01-17T09:01:58.205146Z","shell.execute_reply":"2024-01-17T09:01:58.205168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pyspark.sql.functions import col, udf\nfrom pyspark.sql.types import DoubleType\nfrom math import radians, sin, cos, sqrt, atan2\nfrom pyspark.sql.window import Window\nfrom pyspark.sql import functions as F\n\n# Define the Haversine distance function\ndef haversine(lat1, lon1, lat2, lon2):\n    lat1, lon1, lat2, lon2 = map(radians, [lat1, lon1, lat2, lon2])\n\n    dlat = lat2 - lat1\n    dlon = lon2 - lon1\n\n    a = sin(dlat / 2) ** 2 + cos(lat1) * cos(lat2) * sin(dlon / 2) ** 2\n    c = 2 * atan2(sqrt(a), sqrt(1 - a))\n\n    return 6371.0 * c  # Radius of Earth in kilometers\n\n# Register the Haversine function as a UDF\nhaversine_udf = udf(haversine, DoubleType())\n\n# Alias the columns in the work_reg_cleaned DataFrame to avoid ambiguity\nwork_reg_cleaned = work_reg_cleaned.withColumnRenamed(\"SSO\", \"Work_SSO\")\nwork_reg_cleaned = work_reg_cleaned.withColumnRenamed(\"State\", \"Work_State\")\n# Cross-join the two DataFrames\ncombined_df = final_df_cleaned.crossJoin(work_reg_cleaned)\n\n# Calculate the distance for each combination of coordinates\ncombined_df = combined_df.withColumn(\n    \"distance\",\n    haversine_udf(\n        col(\"Lat\"), col(\"Long\"), col(\"Lattitude\"), col(\"Longitude\")\n    )\n)\n\n# Find the minimum distance for each row in final_df_cleaned\nwindow_spec = Window.partitionBy(\"District\").orderBy(\"distance\")\n\nfinal_df_with_min_distance = combined_df.withColumn(\n    \"min_distance\",\n    F.first(\"distance\").over(window_spec)\n)\n\n# Select the relevant columns and drop duplicates\nfinal_df_with_min_distance = final_df_with_min_distance.select(\n    \"District\", \"Lat\", \"Long\", \"count\", \"SSO\", \"State\", \"min_distance\"\n).distinct()\n\n# Show the resulting DataFrame with the minimum distances\n#final_df_with_min_distance.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-01-17T09:01:58.206761Z","iopub.status.idle":"2024-01-17T09:01:58.207406Z","shell.execute_reply.started":"2024-01-17T09:01:58.207197Z","shell.execute_reply":"2024-01-17T09:01:58.207217Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"csv_path = \"/kaggle/working/your_file.csv\"\n\nfinal_df_with_min_distance.write.csv(csv_path, header=True, mode=\"overwrite\")","metadata":{"execution":{"iopub.status.busy":"2024-01-17T09:01:58.209603Z","iopub.status.idle":"2024-01-17T09:01:58.210266Z","shell.execute_reply.started":"2024-01-17T09:01:58.209951Z","shell.execute_reply":"2024-01-17T09:01:58.209978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pyspark.sql.functions import col, udf\nfrom pyspark.sql.types import DoubleType\nfrom math import radians, sin, cos, sqrt, atan2\nfrom pyspark.sql.window import Window\nfrom pyspark.sql import functions as F\n\n# Define the Haversine distance function\ndef haversine(lat1, lon1, lat2, lon2):\n    # Ensure that the input values are numeric\n    if isinstance(lat1, str) or isinstance(lon1, str) or isinstance(lat2, str) or isinstance(lon2, str):\n        return None\n    \n    lat1, lon1, lat2, lon2 = map(radians, [lat1, lon1, lat2, lon2])\n\n    dlat = lat2 - lat1\n    dlon = lon2 - lon1\n\n    a = sin(dlat / 2) ** 2 + cos(lat1) * cos(lat2) * sin(dlon / 2) ** 2\n    c = 2 * atan2(sqrt(a), sqrt(1 - a))\n\n    return 6371.0 * c  # Radius of Earth in kilometers\n\n# Register the Haversine function as a UDF\nhaversine_udf = udf(haversine, DoubleType())\n\n# Alias the columns in the work_reg_cleaned DataFrame to avoid ambiguity\nwork_reg_cleaned = work_reg_cleaned.withColumnRenamed(\"SSO\", \"Work_SSO\")\nwork_reg_cleaned = work_reg_cleaned.withColumnRenamed(\"State\", \"Work_State\")\n\n# Ensure that the columns are of numeric type\nfinal_df_cleaned = final_df_cleaned.withColumn(\"Lat\", final_df_cleaned[\"Lat\"].cast(DoubleType()))\nfinal_df_cleaned = final_df_cleaned.withColumn(\"Long\", final_df_cleaned[\"Long\"].cast(DoubleType()))\nwork_reg_cleaned = work_reg_cleaned.withColumn(\"Lattitude\", work_reg_cleaned[\"Lattitude\"].cast(DoubleType()))\nwork_reg_cleaned = work_reg_cleaned.withColumn(\"Longitude\", work_reg_cleaned[\"Longitude\"].cast(DoubleType()))\n\n# Cross-join the two DataFrames\ncombined_df = final_df_cleaned.crossJoin(work_reg_cleaned)\n\n# Calculate the distance for each combination of coordinates\ncombined_df = combined_df.withColumn(\n    \"distance\",\n    haversine_udf(\n        col(\"Lat\"), col(\"Long\"), col(\"Lattitude\"), col(\"Longitude\")\n    )\n)\n\n# Find the minimum distance for each row in final_df_cleaned\nwindow_spec = Window.partitionBy(\"District\").orderBy(\"distance\")\n\nfinal_df_with_min_distance = combined_df.withColumn(\n    \"min_distance\",\n    F.first(\"distance\").over(window_spec)\n)\n\n# Select the relevant columns and drop duplicates\nfinal_df_with_min_distance = final_df_with_min_distance.select(\n    \"District\", \"Lat\", \"Long\", \"count\", \"SSO\", \"State\", \"min_distance\"\n).distinct()\n\n# Show the resulting DataFrame with the minimum distances\nfinal_df_with_min_distance.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-01-17T09:01:58.211784Z","iopub.status.idle":"2024-01-17T09:01:58.212389Z","shell.execute_reply.started":"2024-01-17T09:01:58.212089Z","shell.execute_reply":"2024-01-17T09:01:58.212116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}